=== Training with seed: 0 ===
Using cpu device
Training td3 on BipedalWalker-v3...
Logging to ./tensorboard/td3_BipedalWalker-v3_seed0/TD3_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 843      |
|    ep_rew_mean     | -95.9    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 2007     |
|    time_elapsed    | 1        |
|    total_timesteps | 3373     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 642      |
|    ep_rew_mean     | -100     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 1986     |
|    time_elapsed    | 2        |
|    total_timesteps | 5139     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 580      |
|    ep_rew_mean     | -98.8    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 1959     |
|    time_elapsed    | 3        |
|    total_timesteps | 6955     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-96.41 +/- 44.22
Episode length: 413.20 +/- 593.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 413      |
|    mean_reward     | -96.4    |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 555      |
|    ep_rew_mean     | -99.5    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 1260     |
|    time_elapsed    | 8        |
|    total_timesteps | 10133    |
| train/             |          |
|    actor_loss      | -0.0244  |
|    critic_loss     | 199      |
|    learning_rate   | 0.001    |
|    n_updates       | 132      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 480      |
|    ep_rew_mean     | -104     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 765      |
|    time_elapsed    | 14       |
|    total_timesteps | 10845    |
| train/             |          |
|    actor_loss      | 0.159    |
|    critic_loss     | 0.552    |
|    learning_rate   | 0.001    |
|    n_updates       | 844      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 446      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 439      |
|    time_elapsed    | 27       |
|    total_timesteps | 11949    |
| train/             |          |
|    actor_loss      | 0.115    |
|    critic_loss     | 0.709    |
|    learning_rate   | 0.001    |
|    n_updates       | 1948     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 371      |
|    time_elapsed    | 33       |
|    total_timesteps | 12441    |
| train/             |          |
|    actor_loss      | -0.108   |
|    critic_loss     | 0.0707   |
|    learning_rate   | 0.001    |
|    n_updates       | 2440     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 358      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 343      |
|    time_elapsed    | 36       |
|    total_timesteps | 12717    |
| train/             |          |
|    actor_loss      | 0.242    |
|    critic_loss     | 0.239    |
|    learning_rate   | 0.001    |
|    n_updates       | 2716     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 404      |
|    ep_rew_mean     | -116     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 203      |
|    time_elapsed    | 77       |
|    total_timesteps | 15806    |
| train/             |          |
|    actor_loss      | -0.261   |
|    critic_loss     | 15       |
|    learning_rate   | 0.001    |
|    n_updates       | 5805     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 373      |
|    ep_rew_mean     | -116     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 196      |
|    time_elapsed    | 82       |
|    total_timesteps | 16157    |
| train/             |          |
|    actor_loss      | 0.251    |
|    critic_loss     | 0.246    |
|    learning_rate   | 0.001    |
|    n_updates       | 6156     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 348      |
|    ep_rew_mean     | -117     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 188      |
|    time_elapsed    | 87       |
|    total_timesteps | 16554    |
| train/             |          |
|    actor_loss      | 1.98     |
|    critic_loss     | 1.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 6553     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 357      |
|    ep_rew_mean     | -116     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 161      |
|    time_elapsed    | 113      |
|    total_timesteps | 18368    |
| train/             |          |
|    actor_loss      | 0.627    |
|    critic_loss     | 0.791    |
|    learning_rate   | 0.001    |
|    n_updates       | 8367     |
---------------------------------
Eval num_timesteps=20000, episode_reward=-108.61 +/- 2.25
Episode length: 77.40 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.4     |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -0.413   |
|    critic_loss     | 1.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 9999     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 363      |
|    ep_rew_mean     | -115     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 144      |
|    time_elapsed    | 139      |
|    total_timesteps | 20165    |
| train/             |          |
|    actor_loss      | -0.704   |
|    critic_loss     | 0.166    |
|    learning_rate   | 0.001    |
|    n_updates       | 10164    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 397      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 124      |
|    time_elapsed    | 188      |
|    total_timesteps | 23508    |
| train/             |          |
|    actor_loss      | 0.656    |
|    critic_loss     | 0.841    |
|    learning_rate   | 0.001    |
|    n_updates       | 13507    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 401      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 115      |
|    time_elapsed    | 219      |
|    total_timesteps | 25360    |
| train/             |          |
|    actor_loss      | 1.61     |
|    critic_loss     | 1.44     |
|    learning_rate   | 0.001    |
|    n_updates       | 15359    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 380      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 114      |
|    time_elapsed    | 222      |
|    total_timesteps | 25619    |
| train/             |          |
|    actor_loss      | 0.385    |
|    critic_loss     | 0.36     |
|    learning_rate   | 0.001    |
|    n_updates       | 15618    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 362      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 114      |
|    time_elapsed    | 227      |
|    total_timesteps | 25911    |
| train/             |          |
|    actor_loss      | 1.65     |
|    critic_loss     | 3.27     |
|    learning_rate   | 0.001    |
|    n_updates       | 15910    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 347      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 112      |
|    time_elapsed    | 232      |
|    total_timesteps | 26252    |
| train/             |          |
|    actor_loss      | 2.06     |
|    critic_loss     | 0.534    |
|    learning_rate   | 0.001    |
|    n_updates       | 16251    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 343      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 110      |
|    time_elapsed    | 248      |
|    total_timesteps | 27374    |
| train/             |          |
|    actor_loss      | 1.1      |
|    critic_loss     | 1.25     |
|    learning_rate   | 0.001    |
|    n_updates       | 17373    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 330      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 109      |
|    time_elapsed    | 252      |
|    total_timesteps | 27668    |
| train/             |          |
|    actor_loss      | 3.02     |
|    critic_loss     | 0.31     |
|    learning_rate   | 0.001    |
|    n_updates       | 17667    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 318      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 108      |
|    time_elapsed    | 257      |
|    total_timesteps | 27996    |
| train/             |          |
|    actor_loss      | 0.307    |
|    critic_loss     | 6.33     |
|    learning_rate   | 0.001    |
|    n_updates       | 17995    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 312      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 107      |
|    time_elapsed    | 268      |
|    total_timesteps | 28733    |
| train/             |          |
|    actor_loss      | 0.628    |
|    critic_loss     | 0.705    |
|    learning_rate   | 0.001    |
|    n_updates       | 18732    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-96.81 +/- 0.16
Episode length: 62.40 +/- 2.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.4     |
|    mean_reward     | -96.8    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 3.56     |
|    critic_loss     | 1.68     |
|    learning_rate   | 0.001    |
|    n_updates       | 19999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 303      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 103      |
|    time_elapsed    | 291      |
|    total_timesteps | 30192    |
| train/             |          |
|    actor_loss      | 1.87     |
|    critic_loss     | 1.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 20191    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 309      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 100      |
|    time_elapsed    | 318      |
|    total_timesteps | 31942    |
| train/             |          |
|    actor_loss      | 2.49     |
|    critic_loss     | 0.445    |
|    learning_rate   | 0.001    |
|    n_updates       | 21941    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 299      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 99       |
|    time_elapsed    | 323      |
|    total_timesteps | 32230    |
| train/             |          |
|    actor_loss      | 3.34     |
|    critic_loss     | 1.88     |
|    learning_rate   | 0.001    |
|    n_updates       | 22229    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 268      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 99       |
|    time_elapsed    | 327      |
|    total_timesteps | 32529    |
| train/             |          |
|    actor_loss      | 1.54     |
|    critic_loss     | 0.922    |
|    learning_rate   | 0.001    |
|    n_updates       | 22528    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 253      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 98       |
|    time_elapsed    | 331      |
|    total_timesteps | 32778    |
| train/             |          |
|    actor_loss      | 2.63     |
|    critic_loss     | 0.568    |
|    learning_rate   | 0.001    |
|    n_updates       | 22777    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 240      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 98       |
|    time_elapsed    | 337      |
|    total_timesteps | 33228    |
| train/             |          |
|    actor_loss      | 4.81     |
|    critic_loss     | 1.5      |
|    learning_rate   | 0.001    |
|    n_updates       | 23227    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 223      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 98       |
|    time_elapsed    | 341      |
|    total_timesteps | 33516    |
| train/             |          |
|    actor_loss      | 3.6      |
|    critic_loss     | 1.19     |
|    learning_rate   | 0.001    |
|    n_updates       | 23515    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 219      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 97       |
|    time_elapsed    | 346      |
|    total_timesteps | 33857    |
| train/             |          |
|    actor_loss      | 2.04     |
|    critic_loss     | 1.54     |
|    learning_rate   | 0.001    |
|    n_updates       | 23856    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 97       |
|    time_elapsed    | 351      |
|    total_timesteps | 34208    |
| train/             |          |
|    actor_loss      | 3.71     |
|    critic_loss     | 0.86     |
|    learning_rate   | 0.001    |
|    n_updates       | 24207    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 212      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 96       |
|    time_elapsed    | 358      |
|    total_timesteps | 34698    |
| train/             |          |
|    actor_loss      | 3.19     |
|    critic_loss     | 0.398    |
|    learning_rate   | 0.001    |
|    n_updates       | 24697    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 213      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 96       |
|    time_elapsed    | 364      |
|    total_timesteps | 35066    |
| train/             |          |
|    actor_loss      | 5.82     |
|    critic_loss     | 0.909    |
|    learning_rate   | 0.001    |
|    n_updates       | 25065    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 186      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 95       |
|    time_elapsed    | 370      |
|    total_timesteps | 35499    |
| train/             |          |
|    actor_loss      | 8.34     |
|    critic_loss     | 1.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 25498    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 190      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 95       |
|    time_elapsed    | 380      |
|    total_timesteps | 36213    |
| train/             |          |
|    actor_loss      | 3.82     |
|    critic_loss     | 0.872    |
|    learning_rate   | 0.001    |
|    n_updates       | 26212    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 196      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 94       |
|    time_elapsed    | 395      |
|    total_timesteps | 37199    |
| train/             |          |
|    actor_loss      | 3.01     |
|    critic_loss     | 1.36     |
|    learning_rate   | 0.001    |
|    n_updates       | 27198    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-104.57 +/- 3.96
Episode length: 416.80 +/- 596.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 417      |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 4.55     |
|    critic_loss     | 5.18     |
|    learning_rate   | 0.001    |
|    n_updates       | 29999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 192      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 91       |
|    time_elapsed    | 439      |
|    total_timesteps | 40163    |
| train/             |          |
|    actor_loss      | 10       |
|    critic_loss     | 4.67     |
|    learning_rate   | 0.001    |
|    n_updates       | 30162    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 194      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 152      |
|    fps             | 89       |
|    time_elapsed    | 469      |
|    total_timesteps | 42180    |
| train/             |          |
|    actor_loss      | 5.32     |
|    critic_loss     | 2.01     |
|    learning_rate   | 0.001    |
|    n_updates       | 32179    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 164      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 89       |
|    time_elapsed    | 473      |
|    total_timesteps | 42513    |
| train/             |          |
|    actor_loss      | 5.3      |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.001    |
|    n_updates       | 32512    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 179      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 160      |
|    fps             | 87       |
|    time_elapsed    | 522      |
|    total_timesteps | 45889    |
| train/             |          |
|    actor_loss      | 8.44     |
|    critic_loss     | 3.4      |
|    learning_rate   | 0.001    |
|    n_updates       | 35888    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-87.07 +/- 8.29
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -87.1    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 6.34     |
|    critic_loss     | 1.47     |
|    learning_rate   | 0.001    |
|    n_updates       | 39999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 241      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 83       |
|    time_elapsed    | 634      |
|    total_timesteps | 53200    |
| train/             |          |
|    actor_loss      | 5.02     |
|    critic_loss     | 2.04     |
|    learning_rate   | 0.001    |
|    n_updates       | 43199    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 280      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 168      |
|    fps             | 82       |
|    time_elapsed    | 699      |
|    total_timesteps | 57382    |
| train/             |          |
|    actor_loss      | 5.27     |
|    critic_loss     | 1.44     |
|    learning_rate   | 0.001    |
|    n_updates       | 47381    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-157.57 +/- 36.47
Episode length: 1288.60 +/- 542.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.29e+03 |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 4.1      |
|    critic_loss     | 0.728    |
|    learning_rate   | 0.001    |
|    n_updates       | 49999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 340      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 172      |
|    fps             | 78       |
|    time_elapsed    | 822      |
|    total_timesteps | 64800    |
| train/             |          |
|    actor_loss      | 4.79     |
|    critic_loss     | 0.548    |
|    learning_rate   | 0.001    |
|    n_updates       | 54799    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-90.59 +/- 26.68
Episode length: 1352.60 +/- 494.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.35e+03 |
|    mean_reward     | -90.6    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 7.35     |
|    critic_loss     | 0.765    |
|    learning_rate   | 0.001    |
|    n_updates       | 59999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 380      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 176      |
|    fps             | 76       |
|    time_elapsed    | 919      |
|    total_timesteps | 70275    |
| train/             |          |
|    actor_loss      | 5.84     |
|    critic_loss     | 0.516    |
|    learning_rate   | 0.001    |
|    n_updates       | 60274    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 426      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 180      |
|    fps             | 75       |
|    time_elapsed    | 1001     |
|    total_timesteps | 75162    |
| train/             |          |
|    actor_loss      | 4.81     |
|    critic_loss     | 0.535    |
|    learning_rate   | 0.001    |
|    n_updates       | 65161    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-60.59 +/- 23.65
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -60.6    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 5.91     |
|    critic_loss     | 0.522    |
|    learning_rate   | 0.001    |
|    n_updates       | 69999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 184      |
|    fps             | 72       |
|    time_elapsed    | 1117     |
|    total_timesteps | 81600    |
| train/             |          |
|    actor_loss      | 5.94     |
|    critic_loss     | 0.361    |
|    learning_rate   | 0.001    |
|    n_updates       | 71599    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 528      |
|    ep_rew_mean     | -106     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 72       |
|    time_elapsed    | 1200     |
|    total_timesteps | 86496    |
| train/             |          |
|    actor_loss      | 4.15     |
|    critic_loss     | 1.06     |
|    learning_rate   | 0.001    |
|    n_updates       | 76495    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-85.38 +/- 0.31
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -85.4    |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 4.81     |
|    critic_loss     | 0.452    |
|    learning_rate   | 0.001    |
|    n_updates       | 79999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 588      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 192      |
|    fps             | 70       |
|    time_elapsed    | 1318     |
|    total_timesteps | 93200    |
| train/             |          |
|    actor_loss      | 5.75     |
|    critic_loss     | 0.743    |
|    learning_rate   | 0.001    |
|    n_updates       | 83199    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 634      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    episodes        | 196      |
|    fps             | 69       |
|    time_elapsed    | 1424     |
|    total_timesteps | 99600    |
| train/             |          |
|    actor_loss      | 7.02     |
|    critic_loss     | 0.296    |
|    learning_rate   | 0.001    |
|    n_updates       | 89599    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-80.74 +/- 17.33
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -80.7    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 6.74     |
|    critic_loss     | 0.314    |
|    learning_rate   | 0.001    |
|    n_updates       | 89999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 696      |
|    ep_rew_mean     | -102     |
| time/              |          |
|    episodes        | 200      |
|    fps             | 68       |
|    time_elapsed    | 1547     |
|    total_timesteps | 106400   |
| train/             |          |
|    actor_loss      | 3.39     |
|    critic_loss     | 0.298    |
|    learning_rate   | 0.001    |
|    n_updates       | 96399    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-70.50 +/- 10.30
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -70.5    |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 4.07     |
|    critic_loss     | 0.433    |
|    learning_rate   | 0.001    |
|    n_updates       | 99999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 757      |
|    ep_rew_mean     | -100     |
| time/              |          |
|    episodes        | 204      |
|    fps             | 67       |
|    time_elapsed    | 1674     |
|    total_timesteps | 113200   |
| train/             |          |
|    actor_loss      | 5.07     |
|    critic_loss     | 0.579    |
|    learning_rate   | 0.001    |
|    n_updates       | 103199   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 818      |
|    ep_rew_mean     | -98.2    |
| time/              |          |
|    episodes        | 208      |
|    fps             | 66       |
|    time_elapsed    | 1787     |
|    total_timesteps | 119600   |
| train/             |          |
|    actor_loss      | 3.21     |
|    critic_loss     | 0.39     |
|    learning_rate   | 0.001    |
|    n_updates       | 109599   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-111.67 +/- 9.41
Episode length: 90.60 +/- 57.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 90.6     |
|    mean_reward     | -112     |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 3.46     |
|    critic_loss     | 0.487    |
|    learning_rate   | 0.001    |
|    n_updates       | 109999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 847      |
|    ep_rew_mean     | -97.3    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 66       |
|    time_elapsed    | 1855     |
|    total_timesteps | 123294   |
| train/             |          |
|    actor_loss      | 3.56     |
|    critic_loss     | 0.283    |
|    learning_rate   | 0.001    |
|    n_updates       | 113293   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 877      |
|    ep_rew_mean     | -96.5    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 66       |
|    time_elapsed    | 1917     |
|    total_timesteps | 126613   |
| train/             |          |
|    actor_loss      | 5.85     |
|    critic_loss     | 2.53     |
|    learning_rate   | 0.001    |
|    n_updates       | 116612   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-38.78 +/- 2.40
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -38.8    |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 4.09     |
|    critic_loss     | 0.658    |
|    learning_rate   | 0.001    |
|    n_updates       | 119999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 937      |
|    ep_rew_mean     | -95.1    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 65       |
|    time_elapsed    | 2046     |
|    total_timesteps | 133200   |
| train/             |          |
|    actor_loss      | 4.12     |
|    critic_loss     | 0.607    |
|    learning_rate   | 0.001    |
|    n_updates       | 123199   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 998      |
|    ep_rew_mean     | -92.8    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 64       |
|    time_elapsed    | 2164     |
|    total_timesteps | 139600   |
| train/             |          |
|    actor_loss      | 2.59     |
|    critic_loss     | 0.738    |
|    learning_rate   | 0.001    |
|    n_updates       | 129599   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-36.00 +/- 11.14
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -36      |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 5.26     |
|    critic_loss     | 0.169    |
|    learning_rate   | 0.001    |
|    n_updates       | 129999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.06e+03 |
|    ep_rew_mean     | -90.2    |
| time/              |          |
|    episodes        | 228      |
|    fps             | 63       |
|    time_elapsed    | 2298     |
|    total_timesteps | 146400   |
| train/             |          |
|    actor_loss      | 5.98     |
|    critic_loss     | 0.175    |
|    learning_rate   | 0.001    |
|    n_updates       | 136399   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-60.86 +/- 2.81
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -60.9    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 2.97     |
|    critic_loss     | 0.361    |
|    learning_rate   | 0.001    |
|    n_updates       | 139999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.12e+03 |
|    ep_rew_mean     | -88.1    |
| time/              |          |
|    episodes        | 232      |
|    fps             | 62       |
|    time_elapsed    | 2432     |
|    total_timesteps | 153200   |
| train/             |          |
|    actor_loss      | 4.04     |
|    critic_loss     | 0.369    |
|    learning_rate   | 0.001    |
|    n_updates       | 143199   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.16e+03 |
|    ep_rew_mean     | -86.6    |
| time/              |          |
|    episodes        | 236      |
|    fps             | 62       |
|    time_elapsed    | 2525     |
|    total_timesteps | 158070   |
| train/             |          |
|    actor_loss      | 3.31     |
|    critic_loss     | 0.335    |
|    learning_rate   | 0.001    |
|    n_updates       | 148069   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-23.81 +/- 36.49
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 3.54     |
|    critic_loss     | 0.176    |
|    learning_rate   | 0.001    |
|    n_updates       | 149999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.19e+03 |
|    ep_rew_mean     | -84.4    |
| time/              |          |
|    episodes        | 240      |
|    fps             | 62       |
|    time_elapsed    | 2613     |
|    total_timesteps | 162235   |
| train/             |          |
|    actor_loss      | 2.45     |
|    critic_loss     | 0.153    |
|    learning_rate   | 0.001    |
|    n_updates       | 152234   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.21e+03 |
|    ep_rew_mean     | -82.8    |
| time/              |          |
|    episodes        | 244      |
|    fps             | 61       |
|    time_elapsed    | 2665     |
|    total_timesteps | 164896   |
| train/             |          |
|    actor_loss      | 4.56     |
|    critic_loss     | 0.619    |
|    learning_rate   | 0.001    |
|    n_updates       | 154895   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.21e+03 |
|    ep_rew_mean     | -81      |
| time/              |          |
|    episodes        | 248      |
|    fps             | 61       |
|    time_elapsed    | 2689     |
|    total_timesteps | 166221   |
| train/             |          |
|    actor_loss      | 4.23     |
|    critic_loss     | 0.428    |
|    learning_rate   | 0.001    |
|    n_updates       | 156220   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.2e+03  |
|    ep_rew_mean     | -80.6    |
| time/              |          |
|    episodes        | 252      |
|    fps             | 61       |
|    time_elapsed    | 2709     |
|    total_timesteps | 167353   |
| train/             |          |
|    actor_loss      | 4.98     |
|    critic_loss     | 0.455    |
|    learning_rate   | 0.001    |
|    n_updates       | 157352   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-76.77 +/- 15.25
Episode length: 431.60 +/- 450.54
---------------------------------
| eval/              |          |
|    mean_ep_length  | 432      |
|    mean_reward     | -76.8    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 4.77     |
|    critic_loss     | 1.31     |
|    learning_rate   | 0.001    |
|    n_updates       | 159999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.25e+03 |
|    ep_rew_mean     | -77.1    |
| time/              |          |
|    episodes        | 256      |
|    fps             | 61       |
|    time_elapsed    | 2820     |
|    total_timesteps | 173200   |
| train/             |          |
|    actor_loss      | 3.14     |
|    critic_loss     | 0.497    |
|    learning_rate   | 0.001    |
|    n_updates       | 163199   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.27e+03 |
|    ep_rew_mean     | -74      |
| time/              |          |
|    episodes        | 260      |
|    fps             | 61       |
|    time_elapsed    | 2918     |
|    total_timesteps | 178605   |
| train/             |          |
|    actor_loss      | 3.3      |
|    critic_loss     | 0.0728   |
|    learning_rate   | 0.001    |
|    n_updates       | 168604   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-21.49 +/- 14.39
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -21.5    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 3.81     |
|    critic_loss     | 0.294    |
|    learning_rate   | 0.001    |
|    n_updates       | 169999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.25e+03 |
|    ep_rew_mean     | -71.3    |
| time/              |          |
|    episodes        | 264      |
|    fps             | 60       |
|    time_elapsed    | 3009     |
|    total_timesteps | 183311   |
| train/             |          |
|    actor_loss      | 4.47     |
|    critic_loss     | 0.532    |
|    learning_rate   | 0.001    |
|    n_updates       | 173310   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.25e+03 |
|    ep_rew_mean     | -68.5    |
| time/              |          |
|    episodes        | 268      |
|    fps             | 60       |
|    time_elapsed    | 3085     |
|    total_timesteps | 187578   |
| train/             |          |
|    actor_loss      | 2.67     |
|    critic_loss     | 1.46     |
|    learning_rate   | 0.001    |
|    n_updates       | 177577   |
---------------------------------
Eval num_timesteps=190000, episode_reward=-79.92 +/- 31.39
Episode length: 1304.40 +/- 591.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.3e+03  |
|    mean_reward     | -79.9    |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 2.41     |
|    critic_loss     | 0.437    |
|    learning_rate   | 0.001    |
|    n_updates       | 179999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.23e+03 |
|    ep_rew_mean     | -65.3    |
| time/              |          |
|    episodes        | 272      |
|    fps             | 60       |
|    time_elapsed    | 3195     |
|    total_timesteps | 193301   |
| train/             |          |
|    actor_loss      | 6.5      |
|    critic_loss     | 0.784    |
|    learning_rate   | 0.001    |
|    n_updates       | 183300   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.19e+03 |
|    ep_rew_mean     | -65.7    |
| time/              |          |
|    episodes        | 276      |
|    fps             | 60       |
|    time_elapsed    | 3212     |
|    total_timesteps | 194223   |
| train/             |          |
|    actor_loss      | 2.73     |
|    critic_loss     | 0.673    |
|    learning_rate   | 0.001    |
|    n_updates       | 184222   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.16e+03 |
|    ep_rew_mean     | -64.5    |
| time/              |          |
|    episodes        | 280      |
|    fps             | 60       |
|    time_elapsed    | 3243     |
|    total_timesteps | 195876   |
| train/             |          |
|    actor_loss      | 3.72     |
|    critic_loss     | 1.45     |
|    learning_rate   | 0.001    |
|    n_updates       | 185875   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -61.5    |
| time/              |          |
|    episodes        | 284      |
|    fps             | 60       |
|    time_elapsed    | 3307     |
|    total_timesteps | 199249   |
| train/             |          |
|    actor_loss      | 5.1      |
|    critic_loss     | 2.12     |
|    learning_rate   | 0.001    |
|    n_updates       | 189248   |
---------------------------------
Eval num_timesteps=200000, episode_reward=105.89 +/- 101.56
Episode length: 1307.80 +/- 469.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.31e+03 |
|    mean_reward     | 106      |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 4.28     |
|    critic_loss     | 0.882    |
|    learning_rate   | 0.001    |
|    n_updates       | 189999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.11e+03 |
|    ep_rew_mean     | -57      |
| time/              |          |
|    episodes        | 288      |
|    fps             | 59       |
|    time_elapsed    | 3390     |
|    total_timesteps | 203367   |
| train/             |          |
|    actor_loss      | 4.29     |
|    critic_loss     | 0.715    |
|    learning_rate   | 0.001    |
|    n_updates       | 193366   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.08e+03 |
|    ep_rew_mean     | -52.9    |
| time/              |          |
|    episodes        | 292      |
|    fps             | 59       |
|    time_elapsed    | 3457     |
|    total_timesteps | 206843   |
| train/             |          |
|    actor_loss      | 5.43     |
|    critic_loss     | 0.622    |
|    learning_rate   | 0.001    |
|    n_updates       | 196842   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-93.49 +/- 4.76
Episode length: 206.40 +/- 67.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 206      |
|    mean_reward     | -93.5    |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 4.22     |
|    critic_loss     | 0.169    |
|    learning_rate   | 0.001    |
|    n_updates       | 199999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.06e+03 |
|    ep_rew_mean     | -47.9    |
| time/              |          |
|    episodes        | 296      |
|    fps             | 59       |
|    time_elapsed    | 3537     |
|    total_timesteps | 211026   |
| train/             |          |
|    actor_loss      | 2.8      |
|    critic_loss     | 0.714    |
|    learning_rate   | 0.001    |
|    n_updates       | 201025   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -49      |
| time/              |          |
|    episodes        | 300      |
|    fps             | 59       |
|    time_elapsed    | 3551     |
|    total_timesteps | 211786   |
| train/             |          |
|    actor_loss      | 6.98     |
|    critic_loss     | 0.557    |
|    learning_rate   | 0.001    |
|    n_updates       | 201785   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 992      |
|    ep_rew_mean     | -42.4    |
| time/              |          |
|    episodes        | 304      |
|    fps             | 59       |
|    time_elapsed    | 3650     |
|    total_timesteps | 217055   |
| train/             |          |
|    actor_loss      | 4.76     |
|    critic_loss     | 0.542    |
|    learning_rate   | 0.001    |
|    n_updates       | 207054   |
---------------------------------
Eval num_timesteps=220000, episode_reward=171.65 +/- 113.18
Episode length: 1349.20 +/- 501.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.35e+03 |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 2.53     |
|    critic_loss     | 0.227    |
|    learning_rate   | 0.001    |
|    n_updates       | 209999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 992      |
|    ep_rew_mean     | -33.3    |
| time/              |          |
|    episodes        | 308      |
|    fps             | 59       |
|    time_elapsed    | 3803     |
|    total_timesteps | 224800   |
| train/             |          |
|    actor_loss      | 1.99     |
|    critic_loss     | 0.534    |
|    learning_rate   | 0.001    |
|    n_updates       | 214799   |
---------------------------------
Eval num_timesteps=230000, episode_reward=113.65 +/- 2.75
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | 114      |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 5.35     |
|    critic_loss     | 1.66     |
|    learning_rate   | 0.001    |
|    n_updates       | 219999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -22.3    |
| time/              |          |
|    episodes        | 312      |
|    fps             | 58       |
|    time_elapsed    | 3940     |
|    total_timesteps | 231600   |
| train/             |          |
|    actor_loss      | 0.875    |
|    critic_loss     | 0.476    |
|    learning_rate   | 0.001    |
|    n_updates       | 221599   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.04e+03 |
|    ep_rew_mean     | -15      |
| time/              |          |
|    episodes        | 316      |
|    fps             | 58       |
|    time_elapsed    | 4033     |
|    total_timesteps | 236377   |
| train/             |          |
|    actor_loss      | -0.0952  |
|    critic_loss     | 0.103    |
|    learning_rate   | 0.001    |
|    n_updates       | 226376   |
---------------------------------
Eval num_timesteps=240000, episode_reward=231.58 +/- 11.09
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | 232      |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.998    |
|    critic_loss     | 0.156    |
|    learning_rate   | 0.001    |
|    n_updates       | 229999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.03e+03 |
|    ep_rew_mean     | -5.27    |
| time/              |          |
|    episodes        | 320      |
|    fps             | 58       |
|    time_elapsed    | 4171     |
|    total_timesteps | 243173   |
| train/             |          |
|    actor_loss      | 0.119    |
|    critic_loss     | 0.245    |
|    learning_rate   | 0.001    |
|    n_updates       | 233172   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 0.126    |
| time/              |          |
|    episodes        | 324      |
|    fps             | 58       |
|    time_elapsed    | 4239     |
|    total_timesteps | 246705   |
| train/             |          |
|    actor_loss      | 0.194    |
|    critic_loss     | 0.895    |
|    learning_rate   | 0.001    |
|    n_updates       | 236704   |
---------------------------------
Eval num_timesteps=250000, episode_reward=171.83 +/- 115.92
Episode length: 1136.00 +/- 342.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.14e+03 |
|    mean_reward     | 172      |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -1.19    |
|    critic_loss     | 0.205    |
|    learning_rate   | 0.001    |
|    n_updates       | 239999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 985      |
|    ep_rew_mean     | 8.71     |
| time/              |          |
|    episodes        | 328      |
|    fps             | 57       |
|    time_elapsed    | 4345     |
|    total_timesteps | 251882   |
| train/             |          |
|    actor_loss      | -0.7     |
|    critic_loss     | 0.245    |
|    learning_rate   | 0.001    |
|    n_updates       | 241881   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 964      |
|    ep_rew_mean     | 17       |
| time/              |          |
|    episodes        | 332      |
|    fps             | 57       |
|    time_elapsed    | 4428     |
|    total_timesteps | 256142   |
| train/             |          |
|    actor_loss      | -0.951   |
|    critic_loss     | 0.221    |
|    learning_rate   | 0.001    |
|    n_updates       | 246141   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 928      |
|    ep_rew_mean     | 18.7     |
| time/              |          |
|    episodes        | 336      |
|    fps             | 57       |
|    time_elapsed    | 4453     |
|    total_timesteps | 257425   |
| train/             |          |
|    actor_loss      | -1.13    |
|    critic_loss     | 0.468    |
|    learning_rate   | 0.001    |
|    n_updates       | 247424   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 911      |
|    ep_rew_mean     | 21.3     |
| time/              |          |
|    episodes        | 340      |
|    fps             | 57       |
|    time_elapsed    | 4496     |
|    total_timesteps | 259619   |
| train/             |          |
|    actor_loss      | -1.22    |
|    critic_loss     | 0.415    |
|    learning_rate   | 0.001    |
|    n_updates       | 249618   |
---------------------------------
Eval num_timesteps=260000, episode_reward=286.54 +/- 1.01
Episode length: 1158.60 +/- 18.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.16e+03 |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -1.96    |
|    critic_loss     | 0.915    |
|    learning_rate   | 0.001    |
|    n_updates       | 249999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 920      |
|    ep_rew_mean     | 29.3     |
| time/              |          |
|    episodes        | 344      |
|    fps             | 57       |
|    time_elapsed    | 4577     |
|    total_timesteps | 263506   |
| train/             |          |
|    actor_loss      | -1.56    |
|    critic_loss     | 0.328    |
|    learning_rate   | 0.001    |
|    n_updates       | 253505   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 950      |
|    ep_rew_mean     | 40.2     |
| time/              |          |
|    episodes        | 348      |
|    fps             | 57       |
|    time_elapsed    | 4657     |
|    total_timesteps | 267805   |
| train/             |          |
|    actor_loss      | -2.34    |
|    critic_loss     | 0.118    |
|    learning_rate   | 0.001    |
|    n_updates       | 257804   |
---------------------------------
Eval num_timesteps=270000, episode_reward=10.77 +/- 50.75
Episode length: 496.20 +/- 216.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 496      |
|    mean_reward     | 10.8     |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -2.77    |
|    critic_loss     | 0.681    |
|    learning_rate   | 0.001    |
|    n_updates       | 259999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 993      |
|    ep_rew_mean     | 53.9     |
| time/              |          |
|    episodes        | 352      |
|    fps             | 57       |
|    time_elapsed    | 4786     |
|    total_timesteps | 274086   |
| train/             |          |
|    actor_loss      | -2.86    |
|    critic_loss     | 0.682    |
|    learning_rate   | 0.001    |
|    n_updates       | 264085   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 62.7     |
| time/              |          |
|    episodes        | 356      |
|    fps             | 57       |
|    time_elapsed    | 4873     |
|    total_timesteps | 278448   |
| train/             |          |
|    actor_loss      | -0.799   |
|    critic_loss     | 0.165    |
|    learning_rate   | 0.001    |
|    n_updates       | 268447   |
---------------------------------
Eval num_timesteps=280000, episode_reward=276.25 +/- 0.68
Episode length: 1244.20 +/- 9.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.24e+03 |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -2.99    |
|    critic_loss     | 0.0524   |
|    learning_rate   | 0.001    |
|    n_updates       | 269999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 977      |
|    ep_rew_mean     | 75       |
| time/              |          |
|    episodes        | 360      |
|    fps             | 56       |
|    time_elapsed    | 4978     |
|    total_timesteps | 283455   |
| train/             |          |
|    actor_loss      | -3.21    |
|    critic_loss     | 0.325    |
|    learning_rate   | 0.001    |
|    n_updates       | 273454   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 969      |
|    ep_rew_mean     | 83.5     |
| time/              |          |
|    episodes        | 364      |
|    fps             | 56       |
|    time_elapsed    | 5048     |
|    total_timesteps | 286948   |
| train/             |          |
|    actor_loss      | -3.51    |
|    critic_loss     | 1.77     |
|    learning_rate   | 0.001    |
|    n_updates       | 276947   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 941      |
|    ep_rew_mean     | 86       |
| time/              |          |
|    episodes        | 368      |
|    fps             | 56       |
|    time_elapsed    | 5077     |
|    total_timesteps | 288465   |
| train/             |          |
|    actor_loss      | -4.25    |
|    critic_loss     | 0.0708   |
|    learning_rate   | 0.001    |
|    n_updates       | 278464   |
---------------------------------
Eval num_timesteps=290000, episode_reward=287.86 +/- 1.03
Episode length: 1095.60 +/- 10.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.1e+03  |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -3.46    |
|    critic_loss     | 0.424    |
|    learning_rate   | 0.001    |
|    n_updates       | 279999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 928      |
|    ep_rew_mean     | 93.7     |
| time/              |          |
|    episodes        | 372      |
|    fps             | 56       |
|    time_elapsed    | 5161     |
|    total_timesteps | 292655   |
| train/             |          |
|    actor_loss      | -2.03    |
|    critic_loss     | 0.288    |
|    learning_rate   | 0.001    |
|    n_updates       | 282654   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 954      |
|    ep_rew_mean     | 105      |
| time/              |          |
|    episodes        | 376      |
|    fps             | 56       |
|    time_elapsed    | 5233     |
|    total_timesteps | 296236   |
| train/             |          |
|    actor_loss      | -6.09    |
|    critic_loss     | 0.282    |
|    learning_rate   | 0.001    |
|    n_updates       | 286235   |
---------------------------------
Eval num_timesteps=300000, episode_reward=295.77 +/- 0.42
Episode length: 1026.40 +/- 17.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -4.2     |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.001    |
|    n_updates       | 289999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 979      |
|    ep_rew_mean     | 120      |
| time/              |          |
|    episodes        | 380      |
|    fps             | 56       |
|    time_elapsed    | 5333     |
|    total_timesteps | 301064   |
| train/             |          |
|    actor_loss      | -4.76    |
|    critic_loss     | 0.388    |
|    learning_rate   | 0.001    |
|    n_updates       | 291063   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 984      |
|    ep_rew_mean     | 129      |
| time/              |          |
|    episodes        | 384      |
|    fps             | 56       |
|    time_elapsed    | 5409     |
|    total_timesteps | 304876   |
| train/             |          |
|    actor_loss      | -5.14    |
|    critic_loss     | 0.154    |
|    learning_rate   | 0.001    |
|    n_updates       | 294875   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 996      |
|    ep_rew_mean     | 140      |
| time/              |          |
|    episodes        | 388      |
|    fps             | 56       |
|    time_elapsed    | 5501     |
|    total_timesteps | 309513   |
| train/             |          |
|    actor_loss      | -6.02    |
|    critic_loss     | 0.173    |
|    learning_rate   | 0.001    |
|    n_updates       | 299512   |
---------------------------------
Eval num_timesteps=310000, episode_reward=287.02 +/- 0.70
Episode length: 1077.00 +/- 11.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -5.59    |
|    critic_loss     | 0.447    |
|    learning_rate   | 0.001    |
|    n_updates       | 299999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 150      |
| time/              |          |
|    episodes        | 392      |
|    fps             | 56       |
|    time_elapsed    | 5602     |
|    total_timesteps | 314449   |
| train/             |          |
|    actor_loss      | -3.89    |
|    critic_loss     | 1.18     |
|    learning_rate   | 0.001    |
|    n_updates       | 304448   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 161      |
| time/              |          |
|    episodes        | 396      |
|    fps             | 56       |
|    time_elapsed    | 5685     |
|    total_timesteps | 318677   |
| train/             |          |
|    actor_loss      | -6.71    |
|    critic_loss     | 0.39     |
|    learning_rate   | 0.001    |
|    n_updates       | 308676   |
---------------------------------
Eval num_timesteps=320000, episode_reward=185.25 +/- 58.05
Episode length: 849.20 +/- 64.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 849      |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -6.34    |
|    critic_loss     | 0.897    |
|    learning_rate   | 0.001    |
|    n_updates       | 309999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.04e+03 |
|    ep_rew_mean     | 175      |
| time/              |          |
|    episodes        | 400      |
|    fps             | 55       |
|    time_elapsed    | 5773     |
|    total_timesteps | 322982   |
| train/             |          |
|    actor_loss      | -6.79    |
|    critic_loss     | 0.43     |
|    learning_rate   | 0.001    |
|    n_updates       | 312981   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.03e+03 |
|    ep_rew_mean     | 183      |
| time/              |          |
|    episodes        | 404      |
|    fps             | 55       |
|    time_elapsed    | 5848     |
|    total_timesteps | 326875   |
| train/             |          |
|    actor_loss      | -5.05    |
|    critic_loss     | 1.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 316874   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 989      |
|    ep_rew_mean     | 183      |
| time/              |          |
|    episodes        | 408      |
|    fps             | 55       |
|    time_elapsed    | 5902     |
|    total_timesteps | 329613   |
| train/             |          |
|    actor_loss      | -6.04    |
|    critic_loss     | 0.389    |
|    learning_rate   | 0.001    |
|    n_updates       | 319612   |
---------------------------------
Eval num_timesteps=330000, episode_reward=233.29 +/- 133.35
Episode length: 779.20 +/- 266.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 779      |
|    mean_reward     | 233      |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -5.78    |
|    critic_loss     | 1.59     |
|    learning_rate   | 0.001    |
|    n_updates       | 319999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 957      |
|    ep_rew_mean     | 184      |
| time/              |          |
|    episodes        | 412      |
|    fps             | 55       |
|    time_elapsed    | 5969     |
|    total_timesteps | 332912   |
| train/             |          |
|    actor_loss      | -8.02    |
|    critic_loss     | 0.48     |
|    learning_rate   | 0.001    |
|    n_updates       | 322911   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 935      |
|    ep_rew_mean     | 185      |
| time/              |          |
|    episodes        | 416      |
|    fps             | 55       |
|    time_elapsed    | 6022     |
|    total_timesteps | 335497   |
| train/             |          |
|    actor_loss      | -8.9     |
|    critic_loss     | 2.33     |
|    learning_rate   | 0.001    |
|    n_updates       | 325496   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 910      |
|    ep_rew_mean     | 188      |
| time/              |          |
|    episodes        | 420      |
|    fps             | 55       |
|    time_elapsed    | 6091     |
|    total_timesteps | 338880   |
| train/             |          |
|    actor_loss      | -6.32    |
|    critic_loss     | 3.68     |
|    learning_rate   | 0.001    |
|    n_updates       | 328879   |
---------------------------------
Eval num_timesteps=340000, episode_reward=294.33 +/- 1.56
Episode length: 1043.60 +/- 21.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.04e+03 |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -7.51    |
|    critic_loss     | 0.235    |
|    learning_rate   | 0.001    |
|    n_updates       | 329999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 909      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    episodes        | 424      |
|    fps             | 55       |
|    time_elapsed    | 6169     |
|    total_timesteps | 342593   |
| train/             |          |
|    actor_loss      | -9.29    |
|    critic_loss     | 0.707    |
|    learning_rate   | 0.001    |
|    n_updates       | 332592   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 891      |
|    ep_rew_mean     | 192      |
| time/              |          |
|    episodes        | 428      |
|    fps             | 55       |
|    time_elapsed    | 6227     |
|    total_timesteps | 345456   |
| train/             |          |
|    actor_loss      | -7.93    |
|    critic_loss     | 0.141    |
|    learning_rate   | 0.001    |
|    n_updates       | 335455   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 888      |
|    ep_rew_mean     | 197      |
| time/              |          |
|    episodes        | 432      |
|    fps             | 55       |
|    time_elapsed    | 6304     |
|    total_timesteps | 349395   |
| train/             |          |
|    actor_loss      | -7.91    |
|    critic_loss     | 0.325    |
|    learning_rate   | 0.001    |
|    n_updates       | 339394   |
---------------------------------
Eval num_timesteps=350000, episode_reward=297.37 +/- 0.57
Episode length: 931.60 +/- 7.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 932      |
|    mean_reward     | 297      |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -6.14    |
|    critic_loss     | 1.19     |
|    learning_rate   | 0.001    |
|    n_updates       | 339999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 908      |
|    ep_rew_mean     | 208      |
| time/              |          |
|    episodes        | 436      |
|    fps             | 55       |
|    time_elapsed    | 6386     |
|    total_timesteps | 353231   |
| train/             |          |
|    actor_loss      | -7.44    |
|    critic_loss     | 2.05     |
|    learning_rate   | 0.001    |
|    n_updates       | 343230   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 914      |
|    ep_rew_mean     | 213      |
| time/              |          |
|    episodes        | 440      |
|    fps             | 55       |
|    time_elapsed    | 6442     |
|    total_timesteps | 356034   |
| train/             |          |
|    actor_loss      | -9.49    |
|    critic_loss     | 2.8      |
|    learning_rate   | 0.001    |
|    n_updates       | 346033   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 915      |
|    ep_rew_mean     | 219      |
| time/              |          |
|    episodes        | 444      |
|    fps             | 55       |
|    time_elapsed    | 6514     |
|    total_timesteps | 359669   |
| train/             |          |
|    actor_loss      | -7.94    |
|    critic_loss     | 1.7      |
|    learning_rate   | 0.001    |
|    n_updates       | 349668   |
---------------------------------
Eval num_timesteps=360000, episode_reward=245.72 +/- 107.73
Episode length: 845.80 +/- 191.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 846      |
|    mean_reward     | 246      |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -10.1    |
|    critic_loss     | 3.56     |
|    learning_rate   | 0.001    |
|    n_updates       | 349999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 909      |
|    ep_rew_mean     | 223      |
| time/              |          |
|    episodes        | 448      |
|    fps             | 55       |
|    time_elapsed    | 6600     |
|    total_timesteps | 363752   |
| train/             |          |
|    actor_loss      | -10.4    |
|    critic_loss     | 1.38     |
|    learning_rate   | 0.001    |
|    n_updates       | 353751   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 890      |
|    ep_rew_mean     | 223      |
| time/              |          |
|    episodes        | 452      |
|    fps             | 55       |
|    time_elapsed    | 6670     |
|    total_timesteps | 367256   |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 3.2      |
|    learning_rate   | 0.001    |
|    n_updates       | 357255   |
---------------------------------
Eval num_timesteps=370000, episode_reward=275.25 +/- 56.62
Episode length: 862.40 +/- 40.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 862      |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.287    |
|    learning_rate   | 0.001    |
|    n_updates       | 359999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 881      |
|    ep_rew_mean     | 225      |
| time/              |          |
|    episodes        | 456      |
|    fps             | 54       |
|    time_elapsed    | 6748     |
|    total_timesteps | 370899   |
| train/             |          |
|    actor_loss      | -10.2    |
|    critic_loss     | 1.17     |
|    learning_rate   | 0.001    |
|    n_updates       | 360898   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 861      |
|    ep_rew_mean     | 220      |
| time/              |          |
|    episodes        | 460      |
|    fps             | 54       |
|    time_elapsed    | 6803     |
|    total_timesteps | 373567   |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 0.407    |
|    learning_rate   | 0.001    |
|    n_updates       | 363566   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 862      |
|    ep_rew_mean     | 225      |
| time/              |          |
|    episodes        | 464      |
|    fps             | 54       |
|    time_elapsed    | 6876     |
|    total_timesteps | 377190   |
| train/             |          |
|    actor_loss      | -10      |
|    critic_loss     | 0.115    |
|    learning_rate   | 0.001    |
|    n_updates       | 367189   |
---------------------------------
Eval num_timesteps=380000, episode_reward=302.54 +/- 0.76
Episode length: 852.80 +/- 8.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 853      |
|    mean_reward     | 303      |
| time/              |          |
|    total_timesteps | 380000   |
| train/             |          |
|    actor_loss      | -10.3    |
|    critic_loss     | 0.477    |
|    learning_rate   | 0.001    |
|    n_updates       | 369999   |
---------------------------------
New best mean reward!
Stopping training because the mean reward 302.54  is above the threshold 300
Training complete. Model saved.
Plotting sample efficiency...
Evaluating model...
Seed 0: mean_reward:258.80 +/- 104.20
