=== Training with seed: 42 ===
Using cpu device
Training td3 on BipedalWalker-v3...
Logging to ./tensorboard/td3_BipedalWalker-v3_seed42/TD3_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 450      |
|    ep_rew_mean     | -100     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 1846     |
|    time_elapsed    | 0        |
|    total_timesteps | 1801     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 456      |
|    ep_rew_mean     | -101     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 1859     |
|    time_elapsed    | 1        |
|    total_timesteps | 3652     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 332      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 1851     |
|    time_elapsed    | 2        |
|    total_timesteps | 3980     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 271      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 1846     |
|    time_elapsed    | 2        |
|    total_timesteps | 4339     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 1860     |
|    time_elapsed    | 3        |
|    total_timesteps | 6191     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | -104     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 1873     |
|    time_elapsed    | 5        |
|    total_timesteps | 9590     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 355      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 1870     |
|    time_elapsed    | 5        |
|    total_timesteps | 9953     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-118.39 +/- 0.06
Episode length: 91.80 +/- 0.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 91.8     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 318      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 1381     |
|    time_elapsed    | 7        |
|    total_timesteps | 10215    |
| train/             |          |
|    actor_loss      | 0.0396   |
|    critic_loss     | 2.17     |
|    learning_rate   | 0.001    |
|    n_updates       | 214      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 340      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 426      |
|    time_elapsed    | 28       |
|    total_timesteps | 12273    |
| train/             |          |
|    actor_loss      | 1.37     |
|    critic_loss     | 13.9     |
|    learning_rate   | 0.001    |
|    n_updates       | 2272     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 339      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 306      |
|    time_elapsed    | 44       |
|    total_timesteps | 13618    |
| train/             |          |
|    actor_loss      | 0.631    |
|    critic_loss     | 3.52     |
|    learning_rate   | 0.001    |
|    n_updates       | 3617     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 325      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 267      |
|    time_elapsed    | 53       |
|    total_timesteps | 14353    |
| train/             |          |
|    actor_loss      | 0.752    |
|    critic_loss     | 10.3     |
|    learning_rate   | 0.001    |
|    n_updates       | 4352     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 302      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 259      |
|    time_elapsed    | 56       |
|    total_timesteps | 14553    |
| train/             |          |
|    actor_loss      | 1.94     |
|    critic_loss     | 15.4     |
|    learning_rate   | 0.001    |
|    n_updates       | 4552     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 285      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 248      |
|    time_elapsed    | 59       |
|    total_timesteps | 14848    |
| train/             |          |
|    actor_loss      | 0.115    |
|    critic_loss     | 1.24     |
|    learning_rate   | 0.001    |
|    n_updates       | 4847     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 297      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 203      |
|    time_elapsed    | 81       |
|    total_timesteps | 16671    |
| train/             |          |
|    actor_loss      | 0.234    |
|    critic_loss     | 0.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 6670     |
---------------------------------
Eval num_timesteps=20000, episode_reward=-114.98 +/- 5.61
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 1.23     |
|    critic_loss     | 2.6      |
|    learning_rate   | 0.001    |
|    n_updates       | 9999     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 334      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 152      |
|    time_elapsed    | 132      |
|    total_timesteps | 20108    |
| train/             |          |
|    actor_loss      | 0.975    |
|    critic_loss     | 0.374    |
|    learning_rate   | 0.001    |
|    n_updates       | 10107    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 342      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 141      |
|    time_elapsed    | 155      |
|    total_timesteps | 21957    |
| train/             |          |
|    actor_loss      | 2.29     |
|    critic_loss     | 0.91     |
|    learning_rate   | 0.001    |
|    n_updates       | 11956    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 375      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 127      |
|    time_elapsed    | 200      |
|    total_timesteps | 25601    |
| train/             |          |
|    actor_loss      | 0.937    |
|    critic_loss     | 0.542    |
|    learning_rate   | 0.001    |
|    n_updates       | 15600    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-105.93 +/- 5.64
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 2.91     |
|    critic_loss     | 1.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 19999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 422      |
|    ep_rew_mean     | -115     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 110      |
|    time_elapsed    | 284      |
|    total_timesteps | 31481    |
| train/             |          |
|    actor_loss      | 2.59     |
|    critic_loss     | 0.403    |
|    learning_rate   | 0.001    |
|    n_updates       | 21480    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 464      |
|    ep_rew_mean     | -114     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 103      |
|    time_elapsed    | 350      |
|    total_timesteps | 36367    |
| train/             |          |
|    actor_loss      | 4.97     |
|    critic_loss     | 0.582    |
|    learning_rate   | 0.001    |
|    n_updates       | 26366    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-65.73 +/- 6.08
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -65.7    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 2.08     |
|    critic_loss     | 1.98     |
|    learning_rate   | 0.001    |
|    n_updates       | 29999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 521      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 96       |
|    time_elapsed    | 446      |
|    total_timesteps | 43200    |
| train/             |          |
|    actor_loss      | 2.29     |
|    critic_loss     | 0.225    |
|    learning_rate   | 0.001    |
|    n_updates       | 33199    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 563      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 93       |
|    time_elapsed    | 522      |
|    total_timesteps | 48867    |
| train/             |          |
|    actor_loss      | 4.76     |
|    critic_loss     | 0.96     |
|    learning_rate   | 0.001    |
|    n_updates       | 38866    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-29.32 +/- 5.33
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -29.3    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 2.58     |
|    critic_loss     | 0.158    |
|    learning_rate   | 0.001    |
|    n_updates       | 39999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 610      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 88       |
|    time_elapsed    | 635      |
|    total_timesteps | 56400    |
| train/             |          |
|    actor_loss      | 2.87     |
|    critic_loss     | 0.883    |
|    learning_rate   | 0.001    |
|    n_updates       | 46399    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-60.74 +/- 5.64
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -60.7    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 2.45     |
|    critic_loss     | 0.0862   |
|    learning_rate   | 0.001    |
|    n_updates       | 49999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 653      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 85       |
|    time_elapsed    | 741      |
|    total_timesteps | 63200    |
| train/             |          |
|    actor_loss      | 2.62     |
|    critic_loss     | 0.341    |
|    learning_rate   | 0.001    |
|    n_updates       | 53199    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 693      |
|    ep_rew_mean     | -101     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 83       |
|    time_elapsed    | 837      |
|    total_timesteps | 69600    |
| train/             |          |
|    actor_loss      | 2.21     |
|    critic_loss     | 0.0646   |
|    learning_rate   | 0.001    |
|    n_updates       | 59599    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-61.64 +/- 3.37
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -61.6    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 4.62     |
|    critic_loss     | 0.191    |
|    learning_rate   | 0.001    |
|    n_updates       | 59999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 729      |
|    ep_rew_mean     | -97.1    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 80       |
|    time_elapsed    | 949      |
|    total_timesteps | 76400    |
| train/             |          |
|    actor_loss      | 2.63     |
|    critic_loss     | 0.158    |
|    learning_rate   | 0.001    |
|    n_updates       | 66399    |
---------------------------------
Eval num_timesteps=80000, episode_reward=1.02 +/- 16.66
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | 1.02     |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 2.28     |
|    critic_loss     | 0.0373   |
|    learning_rate   | 0.001    |
|    n_updates       | 69999    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 775      |
|    ep_rew_mean     | -92.8    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 78       |
|    time_elapsed    | 1062     |
|    total_timesteps | 83200    |
| train/             |          |
|    actor_loss      | 2.26     |
|    critic_loss     | 0.719    |
|    learning_rate   | 0.001    |
|    n_updates       | 73199    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 805      |
|    ep_rew_mean     | -90.5    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 77       |
|    time_elapsed    | 1138     |
|    total_timesteps | 88052    |
| train/             |          |
|    actor_loss      | 1.02     |
|    critic_loss     | 0.0867   |
|    learning_rate   | 0.001    |
|    n_updates       | 78051    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-93.17 +/- 12.99
Episode length: 391.40 +/- 604.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 391      |
|    mean_reward     | -93.2    |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 0.898    |
|    critic_loss     | 0.484    |
|    learning_rate   | 0.001    |
|    n_updates       | 79999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 851      |
|    ep_rew_mean     | -88      |
| time/              |          |
|    episodes        | 112      |
|    fps             | 76       |
|    time_elapsed    | 1221     |
|    total_timesteps | 93324    |
| train/             |          |
|    actor_loss      | 1.43     |
|    critic_loss     | 0.14     |
|    learning_rate   | 0.001    |
|    n_updates       | 83323    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 882      |
|    ep_rew_mean     | -86.2    |
| time/              |          |
|    episodes        | 116      |
|    fps             | 75       |
|    time_elapsed    | 1274     |
|    total_timesteps | 96770    |
| train/             |          |
|    actor_loss      | 1.52     |
|    critic_loss     | 0.662    |
|    learning_rate   | 0.001    |
|    n_updates       | 86769    |
---------------------------------
Eval num_timesteps=100000, episode_reward=-94.60 +/- 27.41
Episode length: 386.80 +/- 606.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 387      |
|    mean_reward     | -94.6    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 1.71     |
|    critic_loss     | 0.244    |
|    learning_rate   | 0.001    |
|    n_updates       | 89999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 928      |
|    ep_rew_mean     | -83.1    |
| time/              |          |
|    episodes        | 120      |
|    fps             | 75       |
|    time_elapsed    | 1375     |
|    total_timesteps | 103200   |
| train/             |          |
|    actor_loss      | 2.12     |
|    critic_loss     | 0.315    |
|    learning_rate   | 0.001    |
|    n_updates       | 93199    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 911      |
|    ep_rew_mean     | -82.8    |
| time/              |          |
|    episodes        | 124      |
|    fps             | 74       |
|    time_elapsed    | 1405     |
|    total_timesteps | 104979   |
| train/             |          |
|    actor_loss      | 1.34     |
|    critic_loss     | 0.0849   |
|    learning_rate   | 0.001    |
|    n_updates       | 94978    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 941      |
|    ep_rew_mean     | -81.1    |
| time/              |          |
|    episodes        | 128      |
|    fps             | 74       |
|    time_elapsed    | 1459     |
|    total_timesteps | 108325   |
| train/             |          |
|    actor_loss      | 2.47     |
|    critic_loss     | 0.142    |
|    learning_rate   | 0.001    |
|    n_updates       | 98324    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-65.38 +/- 82.11
Episode length: 1547.40 +/- 105.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.55e+03 |
|    mean_reward     | -65.4    |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 1.97     |
|    critic_loss     | 0.218    |
|    learning_rate   | 0.001    |
|    n_updates       | 99999    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 988      |
|    ep_rew_mean     | -77.7    |
| time/              |          |
|    episodes        | 132      |
|    fps             | 73       |
|    time_elapsed    | 1545     |
|    total_timesteps | 113325   |
| train/             |          |
|    actor_loss      | 0.559    |
|    critic_loss     | 0.0515   |
|    learning_rate   | 0.001    |
|    n_updates       | 103324   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.03e+03 |
|    ep_rew_mean     | -72.9    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 72       |
|    time_elapsed    | 1651     |
|    total_timesteps | 119725   |
| train/             |          |
|    actor_loss      | 1.98     |
|    critic_loss     | 0.179    |
|    learning_rate   | 0.001    |
|    n_updates       | 109724   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-19.18 +/- 15.77
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -19.2    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 1.51     |
|    critic_loss     | 0.272    |
|    learning_rate   | 0.001    |
|    n_updates       | 109999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.07e+03 |
|    ep_rew_mean     | -68.8    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 71       |
|    time_elapsed    | 1742     |
|    total_timesteps | 124848   |
| train/             |          |
|    actor_loss      | 3.16     |
|    critic_loss     | 0.0609   |
|    learning_rate   | 0.001    |
|    n_updates       | 114847   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.11e+03 |
|    ep_rew_mean     | -64.7    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 71       |
|    time_elapsed    | 1823     |
|    total_timesteps | 129700   |
| train/             |          |
|    actor_loss      | 1.7      |
|    critic_loss     | 0.275    |
|    learning_rate   | 0.001    |
|    n_updates       | 119699   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-148.48 +/- 23.47
Episode length: 1296.60 +/- 606.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.3e+03  |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 1.73     |
|    critic_loss     | 0.037    |
|    learning_rate   | 0.001    |
|    n_updates       | 119999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.16e+03 |
|    ep_rew_mean     | -60.3    |
| time/              |          |
|    episodes        | 148      |
|    fps             | 70       |
|    time_elapsed    | 1917     |
|    total_timesteps | 135006   |
| train/             |          |
|    actor_loss      | 1.11     |
|    critic_loss     | 0.281    |
|    learning_rate   | 0.001    |
|    n_updates       | 125005   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-105.78 +/- 1.35
Episode length: 84.60 +/- 18.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 84.6     |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 0.589    |
|    critic_loss     | 0.223    |
|    learning_rate   | 0.001    |
|    n_updates       | 129999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.2e+03  |
|    ep_rew_mean     | -55.3    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 69       |
|    time_elapsed    | 2006     |
|    total_timesteps | 140126   |
| train/             |          |
|    actor_loss      | 0.427    |
|    critic_loss     | 0.798    |
|    learning_rate   | 0.001    |
|    n_updates       | 130125   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.25e+03 |
|    ep_rew_mean     | -47.2    |
| time/              |          |
|    episodes        | 156      |
|    fps             | 69       |
|    time_elapsed    | 2112     |
|    total_timesteps | 146363   |
| train/             |          |
|    actor_loss      | 1.28     |
|    critic_loss     | 0.239    |
|    learning_rate   | 0.001    |
|    n_updates       | 136362   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-15.37 +/- 116.61
Episode length: 726.40 +/- 713.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 726      |
|    mean_reward     | -15.4    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -0.0472  |
|    critic_loss     | 0.0517   |
|    learning_rate   | 0.001    |
|    n_updates       | 139999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.25e+03 |
|    ep_rew_mean     | -46.1    |
| time/              |          |
|    episodes        | 160      |
|    fps             | 68       |
|    time_elapsed    | 2189     |
|    total_timesteps | 150585   |
| train/             |          |
|    actor_loss      | -0.26    |
|    critic_loss     | 0.138    |
|    learning_rate   | 0.001    |
|    n_updates       | 140584   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | -46.1    |
| time/              |          |
|    episodes        | 164      |
|    fps             | 68       |
|    time_elapsed    | 2207     |
|    total_timesteps | 151627   |
| train/             |          |
|    actor_loss      | 0.888    |
|    critic_loss     | 0.245    |
|    learning_rate   | 0.001    |
|    n_updates       | 141626   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.25e+03 |
|    ep_rew_mean     | -40.8    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 68       |
|    time_elapsed    | 2292     |
|    total_timesteps | 156487   |
| train/             |          |
|    actor_loss      | 0.331    |
|    critic_loss     | 0.0499   |
|    learning_rate   | 0.001    |
|    n_updates       | 146486   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | -35      |
| time/              |          |
|    episodes        | 172      |
|    fps             | 67       |
|    time_elapsed    | 2354     |
|    total_timesteps | 159922   |
| train/             |          |
|    actor_loss      | 0.397    |
|    critic_loss     | 0.078    |
|    learning_rate   | 0.001    |
|    n_updates       | 149921   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-118.08 +/- 1.47
Episode length: 52.20 +/- 1.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 52.2     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 1.8      |
|    critic_loss     | 1.02     |
|    learning_rate   | 0.001    |
|    n_updates       | 149999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.25e+03 |
|    ep_rew_mean     | -27      |
| time/              |          |
|    episodes        | 176      |
|    fps             | 67       |
|    time_elapsed    | 2453     |
|    total_timesteps | 165534   |
| train/             |          |
|    actor_loss      | -0.656   |
|    critic_loss     | 0.0697   |
|    learning_rate   | 0.001    |
|    n_updates       | 155533   |
---------------------------------
Eval num_timesteps=170000, episode_reward=229.73 +/- 34.68
Episode length: 1526.00 +/- 66.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.53e+03 |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 0.872    |
|    critic_loss     | 0.538    |
|    learning_rate   | 0.001    |
|    n_updates       | 159999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.23e+03 |
|    ep_rew_mean     | -19.5    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 66       |
|    time_elapsed    | 2568     |
|    total_timesteps | 171600   |
| train/             |          |
|    actor_loss      | 3.84     |
|    critic_loss     | 0.725    |
|    learning_rate   | 0.001    |
|    n_updates       | 161599   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | -8.93    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 66       |
|    time_elapsed    | 2681     |
|    total_timesteps | 178000   |
| train/             |          |
|    actor_loss      | 0.433    |
|    critic_loss     | 0.186    |
|    learning_rate   | 0.001    |
|    n_updates       | 167999   |
---------------------------------
Eval num_timesteps=180000, episode_reward=221.54 +/- 4.89
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | 222      |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 0.198    |
|    critic_loss     | 0.08     |
|    learning_rate   | 0.001    |
|    n_updates       | 169999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | 2.19     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 65       |
|    time_elapsed    | 2811     |
|    total_timesteps | 184784   |
| train/             |          |
|    actor_loss      | 0.283    |
|    critic_loss     | 0.243    |
|    learning_rate   | 0.001    |
|    n_updates       | 174783   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.22e+03 |
|    ep_rew_mean     | 9.57     |
| time/              |          |
|    episodes        | 192      |
|    fps             | 65       |
|    time_elapsed    | 2893     |
|    total_timesteps | 189319   |
| train/             |          |
|    actor_loss      | -1.63    |
|    critic_loss     | 0.0581   |
|    learning_rate   | 0.001    |
|    n_updates       | 179318   |
---------------------------------
Eval num_timesteps=190000, episode_reward=265.25 +/- 2.01
Episode length: 1484.60 +/- 26.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.48e+03 |
|    mean_reward     | 265      |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 1.77     |
|    critic_loss     | 0.0922   |
|    learning_rate   | 0.001    |
|    n_updates       | 179999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.2e+03  |
|    ep_rew_mean     | 17       |
| time/              |          |
|    episodes        | 196      |
|    fps             | 64       |
|    time_elapsed    | 2997     |
|    total_timesteps | 194745   |
| train/             |          |
|    actor_loss      | -2.12    |
|    critic_loss     | 0.15     |
|    learning_rate   | 0.001    |
|    n_updates       | 184744   |
---------------------------------
Eval num_timesteps=200000, episode_reward=261.27 +/- 3.72
Episode length: 1461.20 +/- 45.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.46e+03 |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | -1.83    |
|    critic_loss     | 0.156    |
|    learning_rate   | 0.001    |
|    n_updates       | 189999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.2e+03  |
|    ep_rew_mean     | 27.8     |
| time/              |          |
|    episodes        | 200      |
|    fps             | 64       |
|    time_elapsed    | 3124     |
|    total_timesteps | 201600   |
| train/             |          |
|    actor_loss      | -0.424   |
|    critic_loss     | 0.147    |
|    learning_rate   | 0.001    |
|    n_updates       | 191599   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.18e+03 |
|    ep_rew_mean     | 34.3     |
| time/              |          |
|    episodes        | 204      |
|    fps             | 64       |
|    time_elapsed    | 3201     |
|    total_timesteps | 205742   |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.28     |
|    learning_rate   | 0.001    |
|    n_updates       | 195741   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.15e+03 |
|    ep_rew_mean     | 37.3     |
| time/              |          |
|    episodes        | 208      |
|    fps             | 64       |
|    time_elapsed    | 3244     |
|    total_timesteps | 208063   |
| train/             |          |
|    actor_loss      | -0.117   |
|    critic_loss     | 0.477    |
|    learning_rate   | 0.001    |
|    n_updates       | 198062   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-1.31 +/- 31.79
Episode length: 443.60 +/- 129.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 444      |
|    mean_reward     | -1.31    |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | -2.98    |
|    critic_loss     | 0.108    |
|    learning_rate   | 0.001    |
|    n_updates       | 199999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.14e+03 |
|    ep_rew_mean     | 44.7     |
| time/              |          |
|    episodes        | 212      |
|    fps             | 63       |
|    time_elapsed    | 3320     |
|    total_timesteps | 212071   |
| train/             |          |
|    actor_loss      | -2.25    |
|    critic_loss     | 0.106    |
|    learning_rate   | 0.001    |
|    n_updates       | 202070   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.11e+03 |
|    ep_rew_mean     | 44.1     |
| time/              |          |
|    episodes        | 216      |
|    fps             | 63       |
|    time_elapsed    | 3334     |
|    total_timesteps | 212844   |
| train/             |          |
|    actor_loss      | -2.36    |
|    critic_loss     | 0.158    |
|    learning_rate   | 0.001    |
|    n_updates       | 202843   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.05e+03 |
|    ep_rew_mean     | 42.6     |
| time/              |          |
|    episodes        | 220      |
|    fps             | 63       |
|    time_elapsed    | 3352     |
|    total_timesteps | 213835   |
| train/             |          |
|    actor_loss      | -2.22    |
|    critic_loss     | 0.119    |
|    learning_rate   | 0.001    |
|    n_updates       | 203834   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.05e+03 |
|    ep_rew_mean     | 44.5     |
| time/              |          |
|    episodes        | 224      |
|    fps             | 63       |
|    time_elapsed    | 3372     |
|    total_timesteps | 214938   |
| train/             |          |
|    actor_loss      | -2.77    |
|    critic_loss     | 0.418    |
|    learning_rate   | 0.001    |
|    n_updates       | 204937   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.03e+03 |
|    ep_rew_mean     | 47       |
| time/              |          |
|    episodes        | 228      |
|    fps             | 63       |
|    time_elapsed    | 3404     |
|    total_timesteps | 216716   |
| train/             |          |
|    actor_loss      | -1.79    |
|    critic_loss     | 0.242    |
|    learning_rate   | 0.001    |
|    n_updates       | 206715   |
---------------------------------
Eval num_timesteps=220000, episode_reward=2.58 +/- 62.61
Episode length: 567.00 +/- 283.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 567      |
|    mean_reward     | 2.58     |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 1        |
|    learning_rate   | 0.001    |
|    n_updates       | 209999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.01e+03 |
|    ep_rew_mean     | 51.7     |
| time/              |          |
|    episodes        | 232      |
|    fps             | 63       |
|    time_elapsed    | 3469     |
|    total_timesteps | 220166   |
| train/             |          |
|    actor_loss      | -3.69    |
|    critic_loss     | 0.17     |
|    learning_rate   | 0.001    |
|    n_updates       | 210165   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 983      |
|    ep_rew_mean     | 59.7     |
| time/              |          |
|    episodes        | 236      |
|    fps             | 63       |
|    time_elapsed    | 3536     |
|    total_timesteps | 223862   |
| train/             |          |
|    actor_loss      | -2.99    |
|    critic_loss     | 3.16     |
|    learning_rate   | 0.001    |
|    n_updates       | 213861   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 982      |
|    ep_rew_mean     | 72.5     |
| time/              |          |
|    episodes        | 240      |
|    fps             | 63       |
|    time_elapsed    | 3622     |
|    total_timesteps | 228570   |
| train/             |          |
|    actor_loss      | -3.35    |
|    critic_loss     | 0.418    |
|    learning_rate   | 0.001    |
|    n_updates       | 218569   |
---------------------------------
Eval num_timesteps=230000, episode_reward=22.31 +/- 138.55
Episode length: 478.20 +/- 330.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 478      |
|    mean_reward     | 22.3     |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -3.26    |
|    critic_loss     | 0.077    |
|    learning_rate   | 0.001    |
|    n_updates       | 219999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 967      |
|    ep_rew_mean     | 79.7     |
| time/              |          |
|    episodes        | 244      |
|    fps             | 62       |
|    time_elapsed    | 3692     |
|    total_timesteps | 232236   |
| train/             |          |
|    actor_loss      | -3.59    |
|    critic_loss     | 0.0796   |
|    learning_rate   | 0.001    |
|    n_updates       | 222235   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 956      |
|    ep_rew_mean     | 83.1     |
| time/              |          |
|    episodes        | 248      |
|    fps             | 62       |
|    time_elapsed    | 3762     |
|    total_timesteps | 236164   |
| train/             |          |
|    actor_loss      | -4.14    |
|    critic_loss     | 2.29     |
|    learning_rate   | 0.001    |
|    n_updates       | 226163   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 945      |
|    ep_rew_mean     | 90.6     |
| time/              |          |
|    episodes        | 252      |
|    fps             | 62       |
|    time_elapsed    | 3832     |
|    total_timesteps | 239942   |
| train/             |          |
|    actor_loss      | -1.61    |
|    critic_loss     | 0.0944   |
|    learning_rate   | 0.001    |
|    n_updates       | 229941   |
---------------------------------
Eval num_timesteps=240000, episode_reward=247.68 +/- 84.71
Episode length: 912.80 +/- 134.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 248      |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -4.65    |
|    critic_loss     | 0.249    |
|    learning_rate   | 0.001    |
|    n_updates       | 229999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 926      |
|    ep_rew_mean     | 97       |
| time/              |          |
|    episodes        | 256      |
|    fps             | 62       |
|    time_elapsed    | 3917     |
|    total_timesteps | 244366   |
| train/             |          |
|    actor_loss      | -3.6     |
|    critic_loss     | 0.795    |
|    learning_rate   | 0.001    |
|    n_updates       | 234365   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 931      |
|    ep_rew_mean     | 112      |
| time/              |          |
|    episodes        | 260      |
|    fps             | 62       |
|    time_elapsed    | 3994     |
|    total_timesteps | 248588   |
| train/             |          |
|    actor_loss      | -3.6     |
|    critic_loss     | 0.0815   |
|    learning_rate   | 0.001    |
|    n_updates       | 238587   |
---------------------------------
Eval num_timesteps=250000, episode_reward=209.18 +/- 163.62
Episode length: 819.40 +/- 378.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 819      |
|    mean_reward     | 209      |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -5.73    |
|    critic_loss     | 0.228    |
|    learning_rate   | 0.001    |
|    n_updates       | 239999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 953      |
|    ep_rew_mean     | 123      |
| time/              |          |
|    episodes        | 264      |
|    fps             | 62       |
|    time_elapsed    | 4064     |
|    total_timesteps | 252208   |
| train/             |          |
|    actor_loss      | -5.36    |
|    critic_loss     | 0.342    |
|    learning_rate   | 0.001    |
|    n_updates       | 242207   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 944      |
|    ep_rew_mean     | 135      |
| time/              |          |
|    episodes        | 268      |
|    fps             | 61       |
|    time_elapsed    | 4137     |
|    total_timesteps | 256220   |
| train/             |          |
|    actor_loss      | -5.78    |
|    critic_loss     | 0.22     |
|    learning_rate   | 0.001    |
|    n_updates       | 246219   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 947      |
|    ep_rew_mean     | 146      |
| time/              |          |
|    episodes        | 272      |
|    fps             | 61       |
|    time_elapsed    | 4206     |
|    total_timesteps | 259905   |
| train/             |          |
|    actor_loss      | -5.76    |
|    critic_loss     | 0.285    |
|    learning_rate   | 0.001    |
|    n_updates       | 249904   |
---------------------------------
Eval num_timesteps=260000, episode_reward=158.94 +/- 120.09
Episode length: 631.20 +/- 196.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 631      |
|    mean_reward     | 159      |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | -5.57    |
|    critic_loss     | 0.103    |
|    learning_rate   | 0.001    |
|    n_updates       | 249999   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 922      |
|    ep_rew_mean     | 151      |
| time/              |          |
|    episodes        | 276      |
|    fps             | 61       |
|    time_elapsed    | 4267     |
|    total_timesteps | 263083   |
| train/             |          |
|    actor_loss      | -6.98    |
|    critic_loss     | 0.373    |
|    learning_rate   | 0.001    |
|    n_updates       | 253082   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 910      |
|    ep_rew_mean     | 158      |
| time/              |          |
|    episodes        | 280      |
|    fps             | 61       |
|    time_elapsed    | 4337     |
|    total_timesteps | 266790   |
| train/             |          |
|    actor_loss      | -5.64    |
|    critic_loss     | 0.101    |
|    learning_rate   | 0.001    |
|    n_updates       | 256789   |
---------------------------------
Eval num_timesteps=270000, episode_reward=296.97 +/- 0.81
Episode length: 913.00 +/- 6.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 913      |
|    mean_reward     | 297      |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | -6.36    |
|    critic_loss     | 3.38     |
|    learning_rate   | 0.001    |
|    n_updates       | 259999   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 882      |
|    ep_rew_mean     | 163      |
| time/              |          |
|    episodes        | 284      |
|    fps             | 61       |
|    time_elapsed    | 4416     |
|    total_timesteps | 270884   |
| train/             |          |
|    actor_loss      | -5.98    |
|    critic_loss     | 0.108    |
|    learning_rate   | 0.001    |
|    n_updates       | 260883   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 853      |
|    ep_rew_mean     | 165      |
| time/              |          |
|    episodes        | 288      |
|    fps             | 61       |
|    time_elapsed    | 4482     |
|    total_timesteps | 274426   |
| train/             |          |
|    actor_loss      | -6.02    |
|    critic_loss     | 0.449    |
|    learning_rate   | 0.001    |
|    n_updates       | 264425   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 844      |
|    ep_rew_mean     | 170      |
| time/              |          |
|    episodes        | 292      |
|    fps             | 61       |
|    time_elapsed    | 4548     |
|    total_timesteps | 278029   |
| train/             |          |
|    actor_loss      | -7.12    |
|    critic_loss     | 0.93     |
|    learning_rate   | 0.001    |
|    n_updates       | 268028   |
---------------------------------
Eval num_timesteps=280000, episode_reward=300.51 +/- 0.95
Episode length: 871.80 +/- 5.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 872      |
|    mean_reward     | 301      |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | -6.42    |
|    critic_loss     | 0.381    |
|    learning_rate   | 0.001    |
|    n_updates       | 269999   |
---------------------------------
New best mean reward!
Stopping training because the mean reward 300.51  is above the threshold 300
Training complete. Model saved.
Plotting sample efficiency...
Evaluating model...
Seed 42: mean_reward:300.60 +/- 0.72
