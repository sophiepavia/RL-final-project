=== Training with seed: 2024 ===
Using cpu device
Training sac on BipedalWalker-v3...
Logging to ./tensorboard/sac_BipedalWalker-v3_seed2024/SAC_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 444      |
|    ep_rew_mean     | -102     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 2614     |
|    time_elapsed    | 0        |
|    total_timesteps | 1777     |
| train/             |          |
|    std             | 0.0498   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 448      |
|    ep_rew_mean     | -99.9    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 2504     |
|    time_elapsed    | 1        |
|    total_timesteps | 3586     |
| train/             |          |
|    std             | 0.0498   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 707      |
|    ep_rew_mean     | -96.7    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 3249     |
|    time_elapsed    | 2        |
|    total_timesteps | 8485     |
| train/             |          |
|    std             | 0.0498   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 548      |
|    ep_rew_mean     | -98.4    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 3266     |
|    time_elapsed    | 2        |
|    total_timesteps | 8760     |
| train/             |          |
|    std             | 0.0498   |
---------------------------------
Eval num_timesteps=10000, episode_reward=-91.98 +/- 0.06
Episode length: 107.80 +/- 0.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 108      |
|    mean_reward     | -92      |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 464      |
|    ep_rew_mean     | -102     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 923      |
|    time_elapsed    | 11       |
|    total_timesteps | 10435    |
| train/             |          |
|    actor_loss      | -10.6    |
|    critic_loss     | 14.4     |
|    ent_coef        | 0.719    |
|    ent_coef_loss   | -2.16    |
|    learning_rate   | 0.00073  |
|    n_updates       | 448      |
|    std             | 0.05     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 420      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 416      |
|    time_elapsed    | 26       |
|    total_timesteps | 11233    |
| train/             |          |
|    actor_loss      | -22.4    |
|    critic_loss     | 7.93     |
|    ent_coef        | 0.407    |
|    ent_coef_loss   | -5.7     |
|    learning_rate   | 0.00073  |
|    n_updates       | 1216     |
|    std             | 0.0501   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 383      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 288      |
|    time_elapsed    | 41       |
|    total_timesteps | 11866    |
| train/             |          |
|    actor_loss      | -24.9    |
|    critic_loss     | 2.31     |
|    ent_coef        | 0.257    |
|    ent_coef_loss   | -8.53    |
|    learning_rate   | 0.00073  |
|    n_updates       | 1856     |
|    std             | 0.0501   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 350      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 243      |
|    time_elapsed    | 50       |
|    total_timesteps | 12341    |
| train/             |          |
|    actor_loss      | -23.7    |
|    critic_loss     | 2.86     |
|    ent_coef        | 0.187    |
|    ent_coef_loss   | -9.97    |
|    learning_rate   | 0.00073  |
|    n_updates       | 2304     |
|    std             | 0.05     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 325      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 207      |
|    time_elapsed    | 61       |
|    total_timesteps | 12853    |
| train/             |          |
|    actor_loss      | -21.2    |
|    critic_loss     | 1.98     |
|    ent_coef        | 0.131    |
|    ent_coef_loss   | -11      |
|    learning_rate   | 0.00073  |
|    n_updates       | 2816     |
|    std             | 0.05     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 304      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 182      |
|    time_elapsed    | 73       |
|    total_timesteps | 13320    |
| train/             |          |
|    actor_loss      | -18.4    |
|    critic_loss     | 2.92     |
|    ent_coef        | 0.0943   |
|    ent_coef_loss   | -12.4    |
|    learning_rate   | 0.00073  |
|    n_updates       | 3328     |
|    std             | 0.0498   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 292      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 160      |
|    time_elapsed    | 87       |
|    total_timesteps | 14013    |
| train/             |          |
|    actor_loss      | -15.3    |
|    critic_loss     | 1.95     |
|    ent_coef        | 0.0632   |
|    ent_coef_loss   | -13.3    |
|    learning_rate   | 0.00073  |
|    n_updates       | 3968     |
|    std             | 0.0497   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 284      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 139      |
|    time_elapsed    | 105      |
|    total_timesteps | 14796    |
| train/             |          |
|    actor_loss      | -11.3    |
|    critic_loss     | 1.9      |
|    ent_coef        | 0.0379   |
|    ent_coef_loss   | -12.4    |
|    learning_rate   | 0.00073  |
|    n_updates       | 4800     |
|    std             | 0.0495   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 272      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 130      |
|    time_elapsed    | 116      |
|    total_timesteps | 15298    |
| train/             |          |
|    actor_loss      | -9.48    |
|    critic_loss     | 2.58     |
|    ent_coef        | 0.0286   |
|    ent_coef_loss   | -10.9    |
|    learning_rate   | 0.00073  |
|    n_updates       | 5312     |
|    std             | 0.0493   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 267      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 120      |
|    time_elapsed    | 133      |
|    total_timesteps | 16127    |
| train/             |          |
|    actor_loss      | -6.75    |
|    critic_loss     | 2.78     |
|    ent_coef        | 0.0193   |
|    ent_coef_loss   | -9.12    |
|    learning_rate   | 0.00073  |
|    n_updates       | 6080     |
|    std             | 0.0491   |
---------------------------------
Eval num_timesteps=20000, episode_reward=-103.93 +/- 3.69
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | -1.67    |
|    critic_loss     | 1.41     |
|    ent_coef        | 0.00523  |
|    ent_coef_loss   | -4.07    |
|    learning_rate   | 0.00073  |
|    n_updates       | 9984     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 316      |
|    ep_rew_mean     | -113     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 84       |
|    time_elapsed    | 244      |
|    total_timesteps | 20757    |
| train/             |          |
|    actor_loss      | -0.928   |
|    critic_loss     | 0.744    |
|    ent_coef        | 0.004    |
|    ent_coef_loss   | -3.06    |
|    learning_rate   | 0.00073  |
|    n_updates       | 10752    |
|    std             | 0.0477   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 372      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 72       |
|    time_elapsed    | 353      |
|    total_timesteps | 25663    |
| train/             |          |
|    actor_loss      | 1.63     |
|    critic_loss     | 1.49     |
|    ent_coef        | 0.00194  |
|    ent_coef_loss   | 0.624    |
|    learning_rate   | 0.00073  |
|    n_updates       | 15616    |
|    std             | 0.0466   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 401      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 67       |
|    time_elapsed    | 432      |
|    total_timesteps | 29117    |
| train/             |          |
|    actor_loss      | 1.83     |
|    critic_loss     | 1.2      |
|    ent_coef        | 0.00207  |
|    ent_coef_loss   | 0.00517  |
|    learning_rate   | 0.00073  |
|    n_updates       | 19072    |
|    std             | 0.0461   |
---------------------------------
Eval num_timesteps=30000, episode_reward=-106.21 +/- 3.78
Episode length: 416.80 +/- 591.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 417      |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 2.02     |
|    critic_loss     | 0.931    |
|    ent_coef        | 0.0021   |
|    ent_coef_loss   | 0.571    |
|    learning_rate   | 0.00073  |
|    n_updates       | 19968    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 427      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 62       |
|    time_elapsed    | 533      |
|    total_timesteps | 33478    |
| train/             |          |
|    actor_loss      | 1.99     |
|    critic_loss     | 0.346    |
|    ent_coef        | 0.00168  |
|    ent_coef_loss   | -0.265   |
|    learning_rate   | 0.00073  |
|    n_updates       | 23488    |
|    std             | 0.0454   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 62       |
|    time_elapsed    | 542      |
|    total_timesteps | 33873    |
| train/             |          |
|    actor_loss      | 2.19     |
|    critic_loss     | 0.811    |
|    ent_coef        | 0.00172  |
|    ent_coef_loss   | -0.9     |
|    learning_rate   | 0.00073  |
|    n_updates       | 23872    |
|    std             | 0.0454   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 394      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 62       |
|    time_elapsed    | 549      |
|    total_timesteps | 34195    |
| train/             |          |
|    actor_loss      | 2.4      |
|    critic_loss     | 1.39     |
|    ent_coef        | 0.00169  |
|    ent_coef_loss   | -0.169   |
|    learning_rate   | 0.00073  |
|    n_updates       | 24192    |
|    std             | 0.0453   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 397      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 60       |
|    time_elapsed    | 591      |
|    total_timesteps | 36074    |
| train/             |          |
|    actor_loss      | 3.37     |
|    critic_loss     | 0.906    |
|    ent_coef        | 0.00185  |
|    ent_coef_loss   | 0.665    |
|    learning_rate   | 0.00073  |
|    n_updates       | 26048    |
|    std             | 0.0448   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 384      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 60       |
|    time_elapsed    | 601      |
|    total_timesteps | 36518    |
| train/             |          |
|    actor_loss      | 4.01     |
|    critic_loss     | 0.876    |
|    ent_coef        | 0.00191  |
|    ent_coef_loss   | 0.111    |
|    learning_rate   | 0.00073  |
|    n_updates       | 26496    |
|    std             | 0.0447   |
---------------------------------
Eval num_timesteps=40000, episode_reward=-93.56 +/- 30.44
Episode length: 1019.80 +/- 710.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.02e+03 |
|    mean_reward     | -93.6    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 3.33     |
|    critic_loss     | 0.486    |
|    ent_coef        | 0.0026   |
|    ent_coef_loss   | -0.28    |
|    learning_rate   | 0.00073  |
|    n_updates       | 29952    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 435      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 57       |
|    time_elapsed    | 757      |
|    total_timesteps | 43200    |
| train/             |          |
|    actor_loss      | 2.69     |
|    critic_loss     | 0.855    |
|    ent_coef        | 0.00242  |
|    ent_coef_loss   | -0.858   |
|    learning_rate   | 0.00073  |
|    n_updates       | 33152    |
|    std             | 0.0441   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 469      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 55       |
|    time_elapsed    | 869      |
|    total_timesteps | 48177    |
| train/             |          |
|    actor_loss      | 2.5      |
|    critic_loss     | 0.453    |
|    ent_coef        | 0.00274  |
|    ent_coef_loss   | 0.384    |
|    learning_rate   | 0.00073  |
|    n_updates       | 38144    |
|    std             | 0.0442   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 454      |
|    ep_rew_mean     | -106     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 55       |
|    time_elapsed    | 879      |
|    total_timesteps | 48588    |
| train/             |          |
|    actor_loss      | 2.53     |
|    critic_loss     | 0.597    |
|    ent_coef        | 0.00281  |
|    ent_coef_loss   | -0.301   |
|    learning_rate   | 0.00073  |
|    n_updates       | 38592    |
|    std             | 0.0443   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 439      |
|    ep_rew_mean     | -106     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 55       |
|    time_elapsed    | 885      |
|    total_timesteps | 48883    |
| train/             |          |
|    actor_loss      | 2.61     |
|    critic_loss     | 0.637    |
|    ent_coef        | 0.00279  |
|    ent_coef_loss   | -0.938   |
|    learning_rate   | 0.00073  |
|    n_updates       | 38848    |
|    std             | 0.0443   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 425      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 55       |
|    time_elapsed    | 893      |
|    total_timesteps | 49263    |
| train/             |          |
|    actor_loss      | 3.06     |
|    critic_loss     | 1.39     |
|    ent_coef        | 0.0028   |
|    ent_coef_loss   | -0.595   |
|    learning_rate   | 0.00073  |
|    n_updates       | 39232    |
|    std             | 0.0443   |
---------------------------------
Eval num_timesteps=50000, episode_reward=-102.72 +/- 14.38
Episode length: 724.80 +/- 716.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 725      |
|    mean_reward     | -103     |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 3.69     |
|    critic_loss     | 0.921    |
|    ent_coef        | 0.0027   |
|    ent_coef_loss   | 0.231    |
|    learning_rate   | 0.00073  |
|    n_updates       | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 397      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 54       |
|    time_elapsed    | 955      |
|    total_timesteps | 51816    |
| train/             |          |
|    actor_loss      | 3.73     |
|    critic_loss     | 0.643    |
|    ent_coef        | 0.00315  |
|    ent_coef_loss   | 0.0987   |
|    learning_rate   | 0.00073  |
|    n_updates       | 41792    |
|    std             | 0.0442   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 54       |
|    time_elapsed    | 968      |
|    total_timesteps | 52389    |
| train/             |          |
|    actor_loss      | 3.76     |
|    critic_loss     | 0.487    |
|    ent_coef        | 0.00321  |
|    ent_coef_loss   | 1.12     |
|    learning_rate   | 0.00073  |
|    n_updates       | 42368    |
|    std             | 0.0442   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 458      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 52       |
|    time_elapsed    | 1111     |
|    total_timesteps | 58789    |
| train/             |          |
|    actor_loss      | 3.16     |
|    critic_loss     | 0.445    |
|    ent_coef        | 0.00317  |
|    ent_coef_loss   | -1.37    |
|    learning_rate   | 0.00073  |
|    n_updates       | 48768    |
|    std             | 0.044    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-97.54 +/- 18.72
Episode length: 987.80 +/- 749.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 988      |
|    mean_reward     | -97.5    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 2.99     |
|    critic_loss     | 0.405    |
|    ent_coef        | 0.00273  |
|    ent_coef_loss   | 0.678    |
|    learning_rate   | 0.00073  |
|    n_updates       | 49984    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 51       |
|    time_elapsed    | 1257     |
|    total_timesteps | 65330    |
| train/             |          |
|    actor_loss      | 2.64     |
|    critic_loss     | 0.526    |
|    ent_coef        | 0.00249  |
|    ent_coef_loss   | -0.76    |
|    learning_rate   | 0.00073  |
|    n_updates       | 55296    |
|    std             | 0.0437   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 533      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 51       |
|    time_elapsed    | 1335     |
|    total_timesteps | 68895    |
| train/             |          |
|    actor_loss      | 2.75     |
|    critic_loss     | 0.377    |
|    ent_coef        | 0.00291  |
|    ent_coef_loss   | 0.333    |
|    learning_rate   | 0.00073  |
|    n_updates       | 58880    |
|    std             | 0.0436   |
---------------------------------
Eval num_timesteps=70000, episode_reward=-44.88 +/- 17.33
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -44.9    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 2.7      |
|    critic_loss     | 0.521    |
|    ent_coef        | 0.00302  |
|    ent_coef_loss   | -0.00691 |
|    learning_rate   | 0.00073  |
|    n_updates       | 59968    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 592      |
|    ep_rew_mean     | -100     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 50       |
|    time_elapsed    | 1507     |
|    total_timesteps | 76400    |
| train/             |          |
|    actor_loss      | 2.43     |
|    critic_loss     | 0.212    |
|    ent_coef        | 0.00223  |
|    ent_coef_loss   | 0.891    |
|    learning_rate   | 0.00073  |
|    n_updates       | 66368    |
|    std             | 0.0436   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 593      |
|    ep_rew_mean     | -99.3    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 50       |
|    time_elapsed    | 1522     |
|    total_timesteps | 77040    |
| train/             |          |
|    actor_loss      | 2.46     |
|    critic_loss     | 0.334    |
|    ent_coef        | 0.00226  |
|    ent_coef_loss   | -0.322   |
|    learning_rate   | 0.00073  |
|    n_updates       | 67008    |
|    std             | 0.0435   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 609      |
|    ep_rew_mean     | -98.7    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 50       |
|    time_elapsed    | 1569     |
|    total_timesteps | 79106    |
| train/             |          |
|    actor_loss      | 2.63     |
|    critic_loss     | 0.527    |
|    ent_coef        | 0.00259  |
|    ent_coef_loss   | -1.28    |
|    learning_rate   | 0.00073  |
|    n_updates       | 69120    |
|    std             | 0.0436   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 608      |
|    ep_rew_mean     | -97.8    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 50       |
|    time_elapsed    | 1581     |
|    total_timesteps | 79647    |
| train/             |          |
|    actor_loss      | 2.78     |
|    critic_loss     | 0.337    |
|    ent_coef        | 0.00235  |
|    ent_coef_loss   | -0.476   |
|    learning_rate   | 0.00073  |
|    n_updates       | 69632    |
|    std             | 0.0436   |
---------------------------------
Eval num_timesteps=80000, episode_reward=-90.53 +/- 16.82
Episode length: 735.00 +/- 706.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 735      |
|    mean_reward     | -90.5    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 2.82     |
|    critic_loss     | 0.598    |
|    ent_coef        | 0.00232  |
|    ent_coef_loss   | -1.35    |
|    learning_rate   | 0.00073  |
|    n_updates       | 69952    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 635      |
|    ep_rew_mean     | -96      |
| time/              |          |
|    episodes        | 148      |
|    fps             | 49       |
|    time_elapsed    | 1674     |
|    total_timesteps | 83525    |
| train/             |          |
|    actor_loss      | 3.16     |
|    critic_loss     | 0.246    |
|    ent_coef        | 0.00241  |
|    ent_coef_loss   | 0.537    |
|    learning_rate   | 0.00073  |
|    n_updates       | 73536    |
|    std             | 0.0432   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 694      |
|    ep_rew_mean     | -93.8    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 49       |
|    time_elapsed    | 1820     |
|    total_timesteps | 89925    |
| train/             |          |
|    actor_loss      | 2.78     |
|    critic_loss     | 0.227    |
|    ent_coef        | 0.00224  |
|    ent_coef_loss   | -0.531   |
|    learning_rate   | 0.00073  |
|    n_updates       | 79936    |
|    std             | 0.0426   |
---------------------------------
Eval num_timesteps=90000, episode_reward=-49.79 +/- 36.76
Episode length: 1007.80 +/- 725.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | -49.8    |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 2.86     |
|    critic_loss     | 0.434    |
|    ent_coef        | 0.00223  |
|    ent_coef_loss   | 0.254    |
|    learning_rate   | 0.00073  |
|    n_updates       | 80000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 720      |
|    ep_rew_mean     | -91.7    |
| time/              |          |
|    episodes        | 156      |
|    fps             | 48       |
|    time_elapsed    | 1908     |
|    total_timesteps | 93406    |
| train/             |          |
|    actor_loss      | 3.05     |
|    critic_loss     | 0.175    |
|    ent_coef        | 0.00194  |
|    ent_coef_loss   | -0.531   |
|    learning_rate   | 0.00073  |
|    n_updates       | 83392    |
|    std             | 0.0423   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 745      |
|    ep_rew_mean     | -89      |
| time/              |          |
|    episodes        | 160      |
|    fps             | 48       |
|    time_elapsed    | 2054     |
|    total_timesteps | 99806    |
| train/             |          |
|    actor_loss      | 2.84     |
|    critic_loss     | 0.306    |
|    ent_coef        | 0.00207  |
|    ent_coef_loss   | 0.0707   |
|    learning_rate   | 0.00073  |
|    n_updates       | 89792    |
|    std             | 0.0418   |
---------------------------------
Eval num_timesteps=100000, episode_reward=-65.46 +/- 36.19
Episode length: 981.00 +/- 758.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 981      |
|    mean_reward     | -65.5    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 2.72     |
|    critic_loss     | 0.129    |
|    ent_coef        | 0.00208  |
|    ent_coef_loss   | 0.532    |
|    learning_rate   | 0.00073  |
|    n_updates       | 89984    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 749      |
|    ep_rew_mean     | -88      |
| time/              |          |
|    episodes        | 164      |
|    fps             | 48       |
|    time_elapsed    | 2191     |
|    total_timesteps | 105364   |
| train/             |          |
|    actor_loss      | 2.53     |
|    critic_loss     | 0.184    |
|    ent_coef        | 0.00201  |
|    ent_coef_loss   | 0.0202   |
|    learning_rate   | 0.00073  |
|    n_updates       | 95360    |
|    std             | 0.0414   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-102.56 +/- 35.74
Episode length: 1076.20 +/- 483.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.08e+03 |
|    mean_reward     | -103     |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 2.55     |
|    critic_loss     | 0.326    |
|    ent_coef        | 0.00202  |
|    ent_coef_loss   | -0.652   |
|    learning_rate   | 0.00073  |
|    n_updates       | 99968    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 779      |
|    ep_rew_mean     | -84.5    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 47       |
|    time_elapsed    | 2385     |
|    total_timesteps | 113200   |
| train/             |          |
|    actor_loss      | 2.34     |
|    critic_loss     | 0.106    |
|    ent_coef        | 0.00191  |
|    ent_coef_loss   | 0.159    |
|    learning_rate   | 0.00073  |
|    n_updates       | 103168   |
|    std             | 0.0408   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 806      |
|    ep_rew_mean     | -81.3    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 47       |
|    time_elapsed    | 2534     |
|    total_timesteps | 119382   |
| train/             |          |
|    actor_loss      | 2.29     |
|    critic_loss     | 0.274    |
|    ent_coef        | 0.00205  |
|    ent_coef_loss   | -0.177   |
|    learning_rate   | 0.00073  |
|    n_updates       | 109376   |
|    std             | 0.0406   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-48.44 +/- 2.93
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -48.4    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 2.45     |
|    critic_loss     | 0.239    |
|    ent_coef        | 0.00203  |
|    ent_coef_loss   | 0.767    |
|    learning_rate   | 0.00073  |
|    n_updates       | 109952   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 866      |
|    ep_rew_mean     | -77.5    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 46       |
|    time_elapsed    | 2705     |
|    total_timesteps | 126400   |
| train/             |          |
|    actor_loss      | 2.21     |
|    critic_loss     | 0.208    |
|    ent_coef        | 0.00189  |
|    ent_coef_loss   | 1.29     |
|    learning_rate   | 0.00073  |
|    n_updates       | 116352   |
|    std             | 0.0403   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-10.76 +/- 9.04
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -10.8    |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 1.9      |
|    critic_loss     | 0.155    |
|    ent_coef        | 0.00184  |
|    ent_coef_loss   | -0.508   |
|    learning_rate   | 0.00073  |
|    n_updates       | 120000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 913      |
|    ep_rew_mean     | -74.4    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 46       |
|    time_elapsed    | 2845     |
|    total_timesteps | 131879   |
| train/             |          |
|    actor_loss      | 2.13     |
|    critic_loss     | 0.142    |
|    ent_coef        | 0.00188  |
|    ent_coef_loss   | 0.251    |
|    learning_rate   | 0.00073  |
|    n_updates       | 121856   |
|    std             | 0.0402   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 945      |
|    ep_rew_mean     | -72.3    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 46       |
|    time_elapsed    | 2963     |
|    total_timesteps | 136893   |
| train/             |          |
|    actor_loss      | 1.9      |
|    critic_loss     | 0.628    |
|    ent_coef        | 0.00174  |
|    ent_coef_loss   | -0.216   |
|    learning_rate   | 0.00073  |
|    n_updates       | 126848   |
|    std             | 0.0399   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-69.58 +/- 53.46
Episode length: 1027.40 +/- 701.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.03e+03 |
|    mean_reward     | -69.6    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 1.83     |
|    critic_loss     | 0.52     |
|    ent_coef        | 0.00188  |
|    ent_coef_loss   | 0.25     |
|    learning_rate   | 0.00073  |
|    n_updates       | 129984   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -68.9    |
| time/              |          |
|    episodes        | 188      |
|    fps             | 45       |
|    time_elapsed    | 3157     |
|    total_timesteps | 144800   |
| train/             |          |
|    actor_loss      | 1.76     |
|    critic_loss     | 0.128    |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | 0.159    |
|    learning_rate   | 0.00073  |
|    n_updates       | 134784   |
|    std             | 0.0394   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 993      |
|    ep_rew_mean     | -66      |
| time/              |          |
|    episodes        | 192      |
|    fps             | 45       |
|    time_elapsed    | 3277     |
|    total_timesteps | 149899   |
| train/             |          |
|    actor_loss      | 1.79     |
|    critic_loss     | 0.0805   |
|    ent_coef        | 0.00159  |
|    ent_coef_loss   | 0.384    |
|    learning_rate   | 0.00073  |
|    n_updates       | 139904   |
|    std             | 0.039    |
---------------------------------
Eval num_timesteps=150000, episode_reward=-52.54 +/- 52.07
Episode length: 829.00 +/- 646.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 829      |
|    mean_reward     | -52.5    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 1.8      |
|    critic_loss     | 0.0884   |
|    ent_coef        | 0.00159  |
|    ent_coef_loss   | -0.0435  |
|    learning_rate   | 0.00073  |
|    n_updates       | 139968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 994      |
|    ep_rew_mean     | -64      |
| time/              |          |
|    episodes        | 196      |
|    fps             | 45       |
|    time_elapsed    | 3403     |
|    total_timesteps | 155031   |
| train/             |          |
|    actor_loss      | 1.63     |
|    critic_loss     | 0.44     |
|    ent_coef        | 0.0016   |
|    ent_coef_loss   | 0.0201   |
|    learning_rate   | 0.00073  |
|    n_updates       | 145024   |
|    std             | 0.0387   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -62.1    |
| time/              |          |
|    episodes        | 200      |
|    fps             | 45       |
|    time_elapsed    | 3487     |
|    total_timesteps | 158563   |
| train/             |          |
|    actor_loss      | 1.76     |
|    critic_loss     | 0.121    |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | -1.1     |
|    learning_rate   | 0.00073  |
|    n_updates       | 148544   |
|    std             | 0.0387   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-89.81 +/- 1.06
Episode length: 140.20 +/- 18.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 140      |
|    mean_reward     | -89.8    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 1.76     |
|    critic_loss     | 0.194    |
|    ent_coef        | 0.00173  |
|    ent_coef_loss   | 1.2      |
|    learning_rate   | 0.00073  |
|    n_updates       | 149952   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.06e+03 |
|    ep_rew_mean     | -60.3    |
| time/              |          |
|    episodes        | 204      |
|    fps             | 45       |
|    time_elapsed    | 3606     |
|    total_timesteps | 163395   |
| train/             |          |
|    actor_loss      | 1.71     |
|    critic_loss     | 0.084    |
|    ent_coef        | 0.00162  |
|    ent_coef_loss   | -0.958   |
|    learning_rate   | 0.00073  |
|    n_updates       | 153408   |
|    std             | 0.0384   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.1e+03  |
|    ep_rew_mean     | -57.7    |
| time/              |          |
|    episodes        | 208      |
|    fps             | 45       |
|    time_elapsed    | 3711     |
|    total_timesteps | 167883   |
| train/             |          |
|    actor_loss      | 1.67     |
|    critic_loss     | 0.14     |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | 1.24     |
|    learning_rate   | 0.00073  |
|    n_updates       | 157888   |
|    std             | 0.038    |
---------------------------------
Eval num_timesteps=170000, episode_reward=-29.66 +/- 1.72
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -29.7    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 1.76     |
|    critic_loss     | 0.116    |
|    ent_coef        | 0.00207  |
|    ent_coef_loss   | -0.615   |
|    learning_rate   | 0.00073  |
|    n_updates       | 160000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.11e+03 |
|    ep_rew_mean     | -55.5    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 45       |
|    time_elapsed    | 3844     |
|    total_timesteps | 173200   |
| train/             |          |
|    actor_loss      | 1.61     |
|    critic_loss     | 0.101    |
|    ent_coef        | 0.00172  |
|    ent_coef_loss   | -0.0124  |
|    learning_rate   | 0.00073  |
|    n_updates       | 163200   |
|    std             | 0.0376   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -53.4    |
| time/              |          |
|    episodes        | 216      |
|    fps             | 44       |
|    time_elapsed    | 3906     |
|    total_timesteps | 175788   |
| train/             |          |
|    actor_loss      | 1.7      |
|    critic_loss     | 0.321    |
|    ent_coef        | 0.00203  |
|    ent_coef_loss   | -0.196   |
|    learning_rate   | 0.00073  |
|    n_updates       | 165760   |
|    std             | 0.0376   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-22.90 +/- 12.78
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -22.9    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 1.69     |
|    critic_loss     | 0.216    |
|    ent_coef        | 0.00201  |
|    ent_coef_loss   | -0.64    |
|    learning_rate   | 0.00073  |
|    n_updates       | 169984   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -50.9    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 44       |
|    time_elapsed    | 4088     |
|    total_timesteps | 183200   |
| train/             |          |
|    actor_loss      | 1.71     |
|    critic_loss     | 0.138    |
|    ent_coef        | 0.0019   |
|    ent_coef_loss   | -0.592   |
|    learning_rate   | 0.00073  |
|    n_updates       | 173184   |
|    std             | 0.0375   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.15e+03 |
|    ep_rew_mean     | -47.6    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 44       |
|    time_elapsed    | 4240     |
|    total_timesteps | 189600   |
| train/             |          |
|    actor_loss      | 1.49     |
|    critic_loss     | 0.113    |
|    ent_coef        | 0.00186  |
|    ent_coef_loss   | -0.383   |
|    learning_rate   | 0.00073  |
|    n_updates       | 179584   |
|    std             | 0.0373   |
---------------------------------
Eval num_timesteps=190000, episode_reward=30.89 +/- 36.41
Episode length: 1548.20 +/- 103.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.55e+03 |
|    mean_reward     | 30.9     |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 1.54     |
|    critic_loss     | 0.356    |
|    ent_coef        | 0.00185  |
|    ent_coef_loss   | 0.0094   |
|    learning_rate   | 0.00073  |
|    n_updates       | 179968   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.16e+03 |
|    ep_rew_mean     | -45.8    |
| time/              |          |
|    episodes        | 228      |
|    fps             | 44       |
|    time_elapsed    | 4375     |
|    total_timesteps | 195000   |
| train/             |          |
|    actor_loss      | 1.51     |
|    critic_loss     | 0.105    |
|    ent_coef        | 0.00173  |
|    ent_coef_loss   | -0.576   |
|    learning_rate   | 0.00073  |
|    n_updates       | 184960   |
|    std             | 0.0372   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -45.7    |
| time/              |          |
|    episodes        | 232      |
|    fps             | 44       |
|    time_elapsed    | 4472     |
|    total_timesteps | 198641   |
| train/             |          |
|    actor_loss      | 1.43     |
|    critic_loss     | 0.106    |
|    ent_coef        | 0.00187  |
|    ent_coef_loss   | -0.389   |
|    learning_rate   | 0.00073  |
|    n_updates       | 188608   |
|    std             | 0.037    |
---------------------------------
Eval num_timesteps=200000, episode_reward=12.76 +/- 14.11
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | 12.8     |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 1.64     |
|    critic_loss     | 0.152    |
|    ent_coef        | 0.00182  |
|    ent_coef_loss   | -0.5     |
|    learning_rate   | 0.00073  |
|    n_updates       | 189952   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.19e+03 |
|    ep_rew_mean     | -42      |
| time/              |          |
|    episodes        | 236      |
|    fps             | 44       |
|    time_elapsed    | 4671     |
|    total_timesteps | 206400   |
| train/             |          |
|    actor_loss      | 1.41     |
|    critic_loss     | 0.18     |
|    ent_coef        | 0.00201  |
|    ent_coef_loss   | -0.864   |
|    learning_rate   | 0.00073  |
|    n_updates       | 196352   |
|    std             | 0.0366   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-52.38 +/- 46.89
Episode length: 518.40 +/- 552.18
---------------------------------
| eval/              |          |
|    mean_ep_length  | 518      |
|    mean_reward     | -52.4    |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 1.33     |
|    critic_loss     | 0.116    |
|    ent_coef        | 0.00189  |
|    ent_coef_loss   | 0.51     |
|    learning_rate   | 0.00073  |
|    n_updates       | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.22e+03 |
|    ep_rew_mean     | -38.6    |
| time/              |          |
|    episodes        | 240      |
|    fps             | 44       |
|    time_elapsed    | 4799     |
|    total_timesteps | 211600   |
| train/             |          |
|    actor_loss      | 1.11     |
|    critic_loss     | 0.115    |
|    ent_coef        | 0.0019   |
|    ent_coef_loss   | -0.848   |
|    learning_rate   | 0.00073  |
|    n_updates       | 201600   |
|    std             | 0.0364   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.26e+03 |
|    ep_rew_mean     | -35.1    |
| time/              |          |
|    episodes        | 244      |
|    fps             | 44       |
|    time_elapsed    | 4906     |
|    total_timesteps | 216084   |
| train/             |          |
|    actor_loss      | 1.22     |
|    critic_loss     | 0.141    |
|    ent_coef        | 0.00177  |
|    ent_coef_loss   | -0.822   |
|    learning_rate   | 0.00073  |
|    n_updates       | 206080   |
|    std             | 0.0363   |
---------------------------------
Eval num_timesteps=220000, episode_reward=34.19 +/- 54.48
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | 34.2     |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 1.09     |
|    critic_loss     | 0.117    |
|    ent_coef        | 0.00193  |
|    ent_coef_loss   | 1.35     |
|    learning_rate   | 0.00073  |
|    n_updates       | 209984   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.28e+03 |
|    ep_rew_mean     | -31.7    |
| time/              |          |
|    episodes        | 248      |
|    fps             | 43       |
|    time_elapsed    | 5045     |
|    total_timesteps | 221600   |
| train/             |          |
|    actor_loss      | 1        |
|    critic_loss     | 0.28     |
|    ent_coef        | 0.00173  |
|    ent_coef_loss   | 0.415    |
|    learning_rate   | 0.00073  |
|    n_updates       | 211584   |
|    std             | 0.0363   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.27e+03 |
|    ep_rew_mean     | -30.1    |
| time/              |          |
|    episodes        | 252      |
|    fps             | 43       |
|    time_elapsed    | 5184     |
|    total_timesteps | 227275   |
| train/             |          |
|    actor_loss      | 1.03     |
|    critic_loss     | 0.111    |
|    ent_coef        | 0.00218  |
|    ent_coef_loss   | -0.832   |
|    learning_rate   | 0.00073  |
|    n_updates       | 217280   |
|    std             | 0.0364   |
---------------------------------
Eval num_timesteps=230000, episode_reward=14.85 +/- 82.54
Episode length: 1300.60 +/- 598.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.3e+03  |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 0.929    |
|    critic_loss     | 0.25     |
|    ent_coef        | 0.00243  |
|    ent_coef_loss   | -0.219   |
|    learning_rate   | 0.00073  |
|    n_updates       | 219968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.29e+03 |
|    ep_rew_mean     | -26.3    |
| time/              |          |
|    episodes        | 256      |
|    fps             | 43       |
|    time_elapsed    | 5332     |
|    total_timesteps | 233200   |
| train/             |          |
|    actor_loss      | 1.11     |
|    critic_loss     | 0.112    |
|    ent_coef        | 0.0019   |
|    ent_coef_loss   | 0.696    |
|    learning_rate   | 0.00073  |
|    n_updates       | 223168   |
|    std             | 0.0368   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.29e+03 |
|    ep_rew_mean     | -24.8    |
| time/              |          |
|    episodes        | 260      |
|    fps             | 43       |
|    time_elapsed    | 5488     |
|    total_timesteps | 239600   |
| train/             |          |
|    actor_loss      | 1.01     |
|    critic_loss     | 0.151    |
|    ent_coef        | 0.00187  |
|    ent_coef_loss   | 0.246    |
|    learning_rate   | 0.00073  |
|    n_updates       | 229568   |
|    std             | 0.0366   |
---------------------------------
Eval num_timesteps=240000, episode_reward=56.00 +/- 37.36
Episode length: 1503.40 +/- 193.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.5e+03  |
|    mean_reward     | 56       |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 0.899    |
|    critic_loss     | 0.0998   |
|    ent_coef        | 0.00192  |
|    ent_coef_loss   | -0.0557  |
|    learning_rate   | 0.00073  |
|    n_updates       | 229952   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.29e+03 |
|    ep_rew_mean     | -22.9    |
| time/              |          |
|    episodes        | 264      |
|    fps             | 43       |
|    time_elapsed    | 5618     |
|    total_timesteps | 244800   |
| train/             |          |
|    actor_loss      | 0.963    |
|    critic_loss     | 0.0958   |
|    ent_coef        | 0.00189  |
|    ent_coef_loss   | -1.66    |
|    learning_rate   | 0.00073  |
|    n_updates       | 234752   |
|    std             | 0.0366   |
---------------------------------
Eval num_timesteps=250000, episode_reward=148.75 +/- 131.97
Episode length: 1249.40 +/- 547.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.25e+03 |
|    mean_reward     | 149      |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | 0.686    |
|    critic_loss     | 0.0854   |
|    ent_coef        | 0.00201  |
|    ent_coef_loss   | 0.674    |
|    learning_rate   | 0.00073  |
|    n_updates       | 240000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.29e+03 |
|    ep_rew_mean     | -17.9    |
| time/              |          |
|    episodes        | 268      |
|    fps             | 43       |
|    time_elapsed    | 5788     |
|    total_timesteps | 251600   |
| train/             |          |
|    actor_loss      | 0.663    |
|    critic_loss     | 0.121    |
|    ent_coef        | 0.00205  |
|    ent_coef_loss   | 0.321    |
|    learning_rate   | 0.00073  |
|    n_updates       | 241600   |
|    std             | 0.0365   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.26e+03 |
|    ep_rew_mean     | -15      |
| time/              |          |
|    episodes        | 272      |
|    fps             | 43       |
|    time_elapsed    | 5865     |
|    total_timesteps | 254856   |
| train/             |          |
|    actor_loss      | 0.586    |
|    critic_loss     | 0.312    |
|    ent_coef        | 0.00211  |
|    ent_coef_loss   | -0.755   |
|    learning_rate   | 0.00073  |
|    n_updates       | 244864   |
|    std             | 0.0367   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.23e+03 |
|    ep_rew_mean     | -14.8    |
| time/              |          |
|    episodes        | 276      |
|    fps             | 43       |
|    time_elapsed    | 5952     |
|    total_timesteps | 258515   |
| train/             |          |
|    actor_loss      | 0.706    |
|    critic_loss     | 0.115    |
|    ent_coef        | 0.00212  |
|    ent_coef_loss   | 1.35     |
|    learning_rate   | 0.00073  |
|    n_updates       | 248512   |
|    std             | 0.0366   |
---------------------------------
Eval num_timesteps=260000, episode_reward=41.72 +/- 107.11
Episode length: 790.60 +/- 467.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 791      |
|    mean_reward     | 41.7     |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 0.575    |
|    critic_loss     | 0.141    |
|    ent_coef        | 0.00196  |
|    ent_coef_loss   | -1.07    |
|    learning_rate   | 0.00073  |
|    n_updates       | 249984   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | -9.73    |
| time/              |          |
|    episodes        | 280      |
|    fps             | 43       |
|    time_elapsed    | 6143     |
|    total_timesteps | 266312   |
| train/             |          |
|    actor_loss      | 0.574    |
|    critic_loss     | 0.186    |
|    ent_coef        | 0.00209  |
|    ent_coef_loss   | 0.0029   |
|    learning_rate   | 0.00073  |
|    n_updates       | 256320   |
|    std             | 0.0366   |
---------------------------------
Eval num_timesteps=270000, episode_reward=61.56 +/- 118.40
Episode length: 783.60 +/- 428.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 784      |
|    mean_reward     | 61.6     |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | 0.514    |
|    critic_loss     | 0.214    |
|    ent_coef        | 0.00232  |
|    ent_coef_loss   | 0.686    |
|    learning_rate   | 0.00073  |
|    n_updates       | 259968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.23e+03 |
|    ep_rew_mean     | -6.75    |
| time/              |          |
|    episodes        | 284      |
|    fps             | 43       |
|    time_elapsed    | 6263     |
|    total_timesteps | 271148   |
| train/             |          |
|    actor_loss      | 0.545    |
|    critic_loss     | 0.126    |
|    ent_coef        | 0.00225  |
|    ent_coef_loss   | -1.7     |
|    learning_rate   | 0.00073  |
|    n_updates       | 261120   |
|    std             | 0.0365   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.23e+03 |
|    ep_rew_mean     | 0.837    |
| time/              |          |
|    episodes        | 288      |
|    fps             | 43       |
|    time_elapsed    | 6422     |
|    total_timesteps | 277522   |
| train/             |          |
|    actor_loss      | 0.326    |
|    critic_loss     | 0.173    |
|    ent_coef        | 0.0024   |
|    ent_coef_loss   | -0.99    |
|    learning_rate   | 0.00073  |
|    n_updates       | 267520   |
|    std             | 0.0363   |
---------------------------------
Eval num_timesteps=280000, episode_reward=225.32 +/- 71.45
Episode length: 1513.00 +/- 78.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.51e+03 |
|    mean_reward     | 225      |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 0.302    |
|    critic_loss     | 0.509    |
|    ent_coef        | 0.0023   |
|    ent_coef_loss   | 0.329    |
|    learning_rate   | 0.00073  |
|    n_updates       | 269952   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | 11.8     |
| time/              |          |
|    episodes        | 292      |
|    fps             | 43       |
|    time_elapsed    | 6598     |
|    total_timesteps | 284536   |
| train/             |          |
|    actor_loss      | 0.059    |
|    critic_loss     | 0.201    |
|    ent_coef        | 0.00236  |
|    ent_coef_loss   | -0.368   |
|    learning_rate   | 0.00073  |
|    n_updates       | 274496   |
|    std             | 0.0364   |
---------------------------------
Eval num_timesteps=290000, episode_reward=214.17 +/- 114.49
Episode length: 1392.80 +/- 388.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.39e+03 |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | -0.0134  |
|    critic_loss     | 0.167    |
|    ent_coef        | 0.00244  |
|    ent_coef_loss   | 1.03     |
|    learning_rate   | 0.00073  |
|    n_updates       | 280000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | 18.6     |
| time/              |          |
|    episodes        | 296      |
|    fps             | 43       |
|    time_elapsed    | 6744     |
|    total_timesteps | 290241   |
| train/             |          |
|    actor_loss      | -0.166   |
|    critic_loss     | 0.251    |
|    ent_coef        | 0.00256  |
|    ent_coef_loss   | 0.0607   |
|    learning_rate   | 0.00073  |
|    n_updates       | 280256   |
|    std             | 0.0366   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.25e+03 |
|    ep_rew_mean     | 27.4     |
| time/              |          |
|    episodes        | 300      |
|    fps             | 43       |
|    time_elapsed    | 6865     |
|    total_timesteps | 295264   |
| train/             |          |
|    actor_loss      | -0.176   |
|    critic_loss     | 0.148    |
|    ent_coef        | 0.00224  |
|    ent_coef_loss   | 0.84     |
|    learning_rate   | 0.00073  |
|    n_updates       | 285248   |
|    std             | 0.0364   |
---------------------------------
Eval num_timesteps=300000, episode_reward=214.38 +/- 85.13
Episode length: 1218.60 +/- 160.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | 214      |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -0.488   |
|    critic_loss     | 0.358    |
|    ent_coef        | 0.0026   |
|    ent_coef_loss   | -0.0609  |
|    learning_rate   | 0.00073  |
|    n_updates       | 289984   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.27e+03 |
|    ep_rew_mean     | 38.6     |
| time/              |          |
|    episodes        | 304      |
|    fps             | 42       |
|    time_elapsed    | 7020     |
|    total_timesteps | 301475   |
| train/             |          |
|    actor_loss      | -0.355   |
|    critic_loss     | 0.12     |
|    ent_coef        | 0.00235  |
|    ent_coef_loss   | 0.153    |
|    learning_rate   | 0.00073  |
|    n_updates       | 291456   |
|    std             | 0.0366   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.27e+03 |
|    ep_rew_mean     | 49.5     |
| time/              |          |
|    episodes        | 308      |
|    fps             | 42       |
|    time_elapsed    | 7140     |
|    total_timesteps | 306448   |
| train/             |          |
|    actor_loss      | -0.872   |
|    critic_loss     | 0.207    |
|    ent_coef        | 0.00293  |
|    ent_coef_loss   | -0.0704  |
|    learning_rate   | 0.00073  |
|    n_updates       | 296448   |
|    std             | 0.0366   |
---------------------------------
Eval num_timesteps=310000, episode_reward=289.58 +/- 0.36
Episode length: 1337.40 +/- 17.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.34e+03 |
|    mean_reward     | 290      |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -1.14    |
|    critic_loss     | 0.117    |
|    ent_coef        | 0.00416  |
|    ent_coef_loss   | 0.269    |
|    learning_rate   | 0.00073  |
|    n_updates       | 299968   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.27e+03 |
|    ep_rew_mean     | 57.5     |
| time/              |          |
|    episodes        | 312      |
|    fps             | 42       |
|    time_elapsed    | 7258     |
|    total_timesteps | 311116   |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.132    |
|    ent_coef        | 0.00365  |
|    ent_coef_loss   | -2.36    |
|    learning_rate   | 0.00073  |
|    n_updates       | 301120   |
|    std             | 0.0371   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.29e+03 |
|    ep_rew_mean     | 68.8     |
| time/              |          |
|    episodes        | 316      |
|    fps             | 42       |
|    time_elapsed    | 7373     |
|    total_timesteps | 315849   |
| train/             |          |
|    actor_loss      | -1.68    |
|    critic_loss     | 0.118    |
|    ent_coef        | 0.00336  |
|    ent_coef_loss   | -0.232   |
|    learning_rate   | 0.00073  |
|    n_updates       | 305856   |
|    std             | 0.0377   |
---------------------------------
Eval num_timesteps=320000, episode_reward=294.02 +/- 1.57
Episode length: 1216.60 +/- 16.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.22e+03 |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -2.04    |
|    critic_loss     | 0.0924   |
|    ent_coef        | 0.00388  |
|    ent_coef_loss   | -0.447   |
|    learning_rate   | 0.00073  |
|    n_updates       | 309952   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.27e+03 |
|    ep_rew_mean     | 77.5     |
| time/              |          |
|    episodes        | 320      |
|    fps             | 42       |
|    time_elapsed    | 7490     |
|    total_timesteps | 320474   |
| train/             |          |
|    actor_loss      | -2.18    |
|    critic_loss     | 0.119    |
|    ent_coef        | 0.00387  |
|    ent_coef_loss   | -0.779   |
|    learning_rate   | 0.00073  |
|    n_updates       | 310464   |
|    std             | 0.038    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.26e+03 |
|    ep_rew_mean     | 89.8     |
| time/              |          |
|    episodes        | 324      |
|    fps             | 42       |
|    time_elapsed    | 7613     |
|    total_timesteps | 325385   |
| train/             |          |
|    actor_loss      | -2.19    |
|    critic_loss     | 0.235    |
|    ent_coef        | 0.00374  |
|    ent_coef_loss   | -0.672   |
|    learning_rate   | 0.00073  |
|    n_updates       | 315392   |
|    std             | 0.0382   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.25e+03 |
|    ep_rew_mean     | 100      |
| time/              |          |
|    episodes        | 328      |
|    fps             | 42       |
|    time_elapsed    | 7710     |
|    total_timesteps | 329345   |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.102    |
|    ent_coef        | 0.00372  |
|    ent_coef_loss   | 0.902    |
|    learning_rate   | 0.00073  |
|    n_updates       | 319360   |
|    std             | 0.0383   |
---------------------------------
Eval num_timesteps=330000, episode_reward=296.33 +/- 2.02
Episode length: 1120.80 +/- 23.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.12e+03 |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.103    |
|    ent_coef        | 0.00377  |
|    ent_coef_loss   | 0.0584   |
|    learning_rate   | 0.00073  |
|    n_updates       | 320000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | 107      |
| time/              |          |
|    episodes        | 332      |
|    fps             | 42       |
|    time_elapsed    | 7806     |
|    total_timesteps | 333101   |
| train/             |          |
|    actor_loss      | -2.67    |
|    critic_loss     | 0.106    |
|    ent_coef        | 0.00425  |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.00073  |
|    n_updates       | 323072   |
|    std             | 0.0386   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.21e+03 |
|    ep_rew_mean     | 111      |
| time/              |          |
|    episodes        | 336      |
|    fps             | 42       |
|    time_elapsed    | 7870     |
|    total_timesteps | 335692   |
| train/             |          |
|    actor_loss      | -2.65    |
|    critic_loss     | 0.101    |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | 0.303    |
|    learning_rate   | 0.00073  |
|    n_updates       | 325696   |
|    std             | 0.0385   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.2e+03  |
|    ep_rew_mean     | 120      |
| time/              |          |
|    episodes        | 340      |
|    fps             | 42       |
|    time_elapsed    | 7968     |
|    total_timesteps | 339666   |
| train/             |          |
|    actor_loss      | -2.96    |
|    critic_loss     | 0.138    |
|    ent_coef        | 0.00391  |
|    ent_coef_loss   | -0.732   |
|    learning_rate   | 0.00073  |
|    n_updates       | 329664   |
|    std             | 0.0384   |
---------------------------------
Eval num_timesteps=340000, episode_reward=298.61 +/- 1.14
Episode length: 1062.00 +/- 26.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.06e+03 |
|    mean_reward     | 299      |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -2.88    |
|    critic_loss     | 0.23     |
|    ent_coef        | 0.00377  |
|    ent_coef_loss   | -0.617   |
|    learning_rate   | 0.00073  |
|    n_updates       | 329984   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.19e+03 |
|    ep_rew_mean     | 130      |
| time/              |          |
|    episodes        | 344      |
|    fps             | 42       |
|    time_elapsed    | 8079     |
|    total_timesteps | 344143   |
| train/             |          |
|    actor_loss      | -3.24    |
|    critic_loss     | 0.107    |
|    ent_coef        | 0.0038   |
|    ent_coef_loss   | 0.0465   |
|    learning_rate   | 0.00073  |
|    n_updates       | 334144   |
|    std             | 0.0383   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.17e+03 |
|    ep_rew_mean     | 138      |
| time/              |          |
|    episodes        | 348      |
|    fps             | 42       |
|    time_elapsed    | 8164     |
|    total_timesteps | 347608   |
| train/             |          |
|    actor_loss      | -3.64    |
|    critic_loss     | 0.144    |
|    ent_coef        | 0.0043   |
|    ent_coef_loss   | 0.926    |
|    learning_rate   | 0.00073  |
|    n_updates       | 337600   |
|    std             | 0.0385   |
---------------------------------
Eval num_timesteps=350000, episode_reward=229.14 +/- 148.86
Episode length: 910.20 +/- 352.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 910      |
|    mean_reward     | 229      |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -3.94    |
|    critic_loss     | 0.108    |
|    ent_coef        | 0.00491  |
|    ent_coef_loss   | -0.755   |
|    learning_rate   | 0.00073  |
|    n_updates       | 339968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.14e+03 |
|    ep_rew_mean     | 144      |
| time/              |          |
|    episodes        | 352      |
|    fps             | 42       |
|    time_elapsed    | 8255     |
|    total_timesteps | 351260   |
| train/             |          |
|    actor_loss      | -3.94    |
|    critic_loss     | 0.139    |
|    ent_coef        | 0.00442  |
|    ent_coef_loss   | -0.728   |
|    learning_rate   | 0.00073  |
|    n_updates       | 341248   |
|    std             | 0.0388   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | 155      |
| time/              |          |
|    episodes        | 356      |
|    fps             | 42       |
|    time_elapsed    | 8357     |
|    total_timesteps | 355476   |
| train/             |          |
|    actor_loss      | -4.13    |
|    critic_loss     | 0.11     |
|    ent_coef        | 0.00481  |
|    ent_coef_loss   | 0.455    |
|    learning_rate   | 0.00073  |
|    n_updates       | 345472   |
|    std             | 0.0394   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.1e+03  |
|    ep_rew_mean     | 163      |
| time/              |          |
|    episodes        | 360      |
|    fps             | 42       |
|    time_elapsed    | 8434     |
|    total_timesteps | 358628   |
| train/             |          |
|    actor_loss      | -4.49    |
|    critic_loss     | 0.0965   |
|    ent_coef        | 0.00534  |
|    ent_coef_loss   | -0.0565  |
|    learning_rate   | 0.00073  |
|    n_updates       | 348608   |
|    std             | 0.0393   |
---------------------------------
Eval num_timesteps=360000, episode_reward=276.11 +/- 49.08
Episode length: 982.60 +/- 38.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 983      |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -4.71    |
|    critic_loss     | 0.125    |
|    ent_coef        | 0.00573  |
|    ent_coef_loss   | 0.546    |
|    learning_rate   | 0.00073  |
|    n_updates       | 349952   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.08e+03 |
|    ep_rew_mean     | 171      |
| time/              |          |
|    episodes        | 364      |
|    fps             | 42       |
|    time_elapsed    | 8531     |
|    total_timesteps | 362466   |
| train/             |          |
|    actor_loss      | -4.82    |
|    critic_loss     | 0.165    |
|    ent_coef        | 0.00487  |
|    ent_coef_loss   | -0.727   |
|    learning_rate   | 0.00073  |
|    n_updates       | 352448   |
|    std             | 0.0395   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.05e+03 |
|    ep_rew_mean     | 176      |
| time/              |          |
|    episodes        | 368      |
|    fps             | 42       |
|    time_elapsed    | 8621     |
|    total_timesteps | 366224   |
| train/             |          |
|    actor_loss      | -4.9     |
|    critic_loss     | 0.129    |
|    ent_coef        | 0.00479  |
|    ent_coef_loss   | 0.0215   |
|    learning_rate   | 0.00073  |
|    n_updates       | 356224   |
|    std             | 0.0394   |
---------------------------------
Eval num_timesteps=370000, episode_reward=304.41 +/- 1.70
Episode length: 1009.00 +/- 19.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.01e+03 |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -5.25    |
|    critic_loss     | 0.117    |
|    ent_coef        | 0.00465  |
|    ent_coef_loss   | -0.717   |
|    learning_rate   | 0.00073  |
|    n_updates       | 360000   |
---------------------------------
New best mean reward!
Stopping training because the mean reward 304.41  is above the threshold 300
Training complete. Model saved.
Plotting sample efficiency...
Evaluating model...
Seed 2024: mean_reward:303.74 +/- 1.04
