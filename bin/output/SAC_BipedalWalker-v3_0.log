=== Training with seed: 0 ===
Using cpu device
Training sac on BipedalWalker-v3...
Logging to ./tensorboard/sac_BipedalWalker-v3_seed0/SAC_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 838      |
|    ep_rew_mean     | -90.2    |
| time/              |          |
|    episodes        | 4        |
|    fps             | 4359     |
|    time_elapsed    | 0        |
|    total_timesteps | 3350     |
| train/             |          |
|    std             | 0.0498   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.03e+03 |
|    ep_rew_mean     | -88.7    |
| time/              |          |
|    episodes        | 8        |
|    fps             | 3102     |
|    time_elapsed    | 2        |
|    total_timesteps | 8252     |
| train/             |          |
|    std             | 0.0498   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 713      |
|    ep_rew_mean     | -96.5    |
| time/              |          |
|    episodes        | 12       |
|    fps             | 3057     |
|    time_elapsed    | 2        |
|    total_timesteps | 8551     |
| train/             |          |
|    std             | 0.0498   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 552      |
|    ep_rew_mean     | -98.9    |
| time/              |          |
|    episodes        | 16       |
|    fps             | 3015     |
|    time_elapsed    | 2        |
|    total_timesteps | 8829     |
| train/             |          |
|    std             | 0.0498   |
---------------------------------
Eval num_timesteps=10000, episode_reward=-118.56 +/- 0.33
Episode length: 130.20 +/- 33.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 130      |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 464      |
|    ep_rew_mean     | -102     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 1042     |
|    time_elapsed    | 9        |
|    total_timesteps | 10359    |
| train/             |          |
|    actor_loss      | -6.71    |
|    critic_loss     | 14.3     |
|    ent_coef        | 0.791    |
|    ent_coef_loss   | -1.55    |
|    learning_rate   | 0.00073  |
|    n_updates       | 320      |
|    std             | 0.05     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 408      |
|    ep_rew_mean     | -104     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 547      |
|    time_elapsed    | 19       |
|    total_timesteps | 10874    |
| train/             |          |
|    actor_loss      | -16.7    |
|    critic_loss     | 3.43     |
|    ent_coef        | 0.539    |
|    ent_coef_loss   | -4.03    |
|    learning_rate   | 0.00073  |
|    n_updates       | 832      |
|    std             | 0.05     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 365      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 363      |
|    time_elapsed    | 31       |
|    total_timesteps | 11313    |
| train/             |          |
|    actor_loss      | -20.4    |
|    critic_loss     | 2.95     |
|    ent_coef        | 0.388    |
|    ent_coef_loss   | -6.19    |
|    learning_rate   | 0.00073  |
|    n_updates       | 1280     |
|    std             | 0.0501   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 330      |
|    ep_rew_mean     | -108     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 298      |
|    time_elapsed    | 39       |
|    total_timesteps | 11647    |
| train/             |          |
|    actor_loss      | -21.3    |
|    critic_loss     | 2.12     |
|    ent_coef        | 0.308    |
|    ent_coef_loss   | -7.3     |
|    learning_rate   | 0.00073  |
|    n_updates       | 1600     |
|    std             | 0.0501   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 310      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 225      |
|    time_elapsed    | 54       |
|    total_timesteps | 12255    |
| train/             |          |
|    actor_loss      | -19.5    |
|    critic_loss     | 3.57     |
|    ent_coef        | 0.196    |
|    ent_coef_loss   | -9.69    |
|    learning_rate   | 0.00073  |
|    n_updates       | 2240     |
|    std             | 0.05     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 288      |
|    ep_rew_mean     | -110     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 203      |
|    time_elapsed    | 61       |
|    total_timesteps | 12596    |
| train/             |          |
|    actor_loss      | -18      |
|    critic_loss     | 1.59     |
|    ent_coef        | 0.157    |
|    ent_coef_loss   | -10.7    |
|    learning_rate   | 0.00073  |
|    n_updates       | 2560     |
|    std             | 0.05     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 274      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 173      |
|    time_elapsed    | 75       |
|    total_timesteps | 13155    |
| train/             |          |
|    actor_loss      | -14.1    |
|    critic_loss     | 1.68     |
|    ent_coef        | 0.106    |
|    ent_coef_loss   | -12.3    |
|    learning_rate   | 0.00073  |
|    n_updates       | 3136     |
|    std             | 0.0498   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 274      |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 139      |
|    time_elapsed    | 102      |
|    total_timesteps | 14232    |
| train/             |          |
|    actor_loss      | -8.35    |
|    critic_loss     | 0.962    |
|    ent_coef        | 0.0526   |
|    ent_coef_loss   | -12.5    |
|    learning_rate   | 0.00073  |
|    n_updates       | 4224     |
|    std             | 0.0494   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 347      |
|    ep_rew_mean     | -111     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 87       |
|    time_elapsed    | 218      |
|    total_timesteps | 19137    |
| train/             |          |
|    actor_loss      | 0.0544   |
|    critic_loss     | 0.331    |
|    ent_coef        | 0.00361  |
|    ent_coef_loss   | -8.21    |
|    learning_rate   | 0.00073  |
|    n_updates       | 9152     |
|    std             | 0.0477   |
---------------------------------
Eval num_timesteps=20000, episode_reward=-108.40 +/- 1.99
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 20000    |
| train/             |          |
|    actor_loss      | 0.61     |
|    critic_loss     | 0.243    |
|    ent_coef        | 0.00252  |
|    ent_coef_loss   | -5.66    |
|    learning_rate   | 0.00073  |
|    n_updates       | 9984     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 382      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 71       |
|    time_elapsed    | 324      |
|    total_timesteps | 23325    |
| train/             |          |
|    actor_loss      | 1.77     |
|    critic_loss     | 0.41     |
|    ent_coef        | 0.00166  |
|    ent_coef_loss   | 0.519    |
|    learning_rate   | 0.00073  |
|    n_updates       | 13312    |
|    std             | 0.046    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 360      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 71       |
|    time_elapsed    | 330      |
|    total_timesteps | 23562    |
| train/             |          |
|    actor_loss      | 1.99     |
|    critic_loss     | 0.437    |
|    ent_coef        | 0.00168  |
|    ent_coef_loss   | 1.4      |
|    learning_rate   | 0.00073  |
|    n_updates       | 13568    |
|    std             | 0.046    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 341      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 70       |
|    time_elapsed    | 335      |
|    total_timesteps | 23786    |
| train/             |          |
|    actor_loss      | 2.33     |
|    critic_loss     | 0.384    |
|    ent_coef        | 0.00168  |
|    ent_coef_loss   | 0.208    |
|    learning_rate   | 0.00073  |
|    n_updates       | 13760    |
|    std             | 0.0459   |
---------------------------------
Eval num_timesteps=30000, episode_reward=-42.41 +/- 5.95
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -42.4    |
| time/              |          |
|    total_timesteps | 30000    |
| train/             |          |
|    actor_loss      | 3.09     |
|    critic_loss     | 0.174    |
|    ent_coef        | 0.00188  |
|    ent_coef_loss   | -0.0843  |
|    learning_rate   | 0.00073  |
|    n_updates       | 19968    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 392      |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 60       |
|    time_elapsed    | 494      |
|    total_timesteps | 30140    |
| train/             |          |
|    actor_loss      | 3.06     |
|    critic_loss     | 0.177    |
|    ent_coef        | 0.00187  |
|    ent_coef_loss   | -0.737   |
|    learning_rate   | 0.00073  |
|    n_updates       | 20096    |
|    std             | 0.0438   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 418      |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 58       |
|    time_elapsed    | 578      |
|    total_timesteps | 33609    |
| train/             |          |
|    actor_loss      | 3.18     |
|    critic_loss     | 0.134    |
|    ent_coef        | 0.002    |
|    ent_coef_loss   | 0.712    |
|    learning_rate   | 0.00073  |
|    n_updates       | 23616    |
|    std             | 0.043    |
---------------------------------
Eval num_timesteps=40000, episode_reward=-44.94 +/- 6.05
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -44.9    |
| time/              |          |
|    total_timesteps | 40000    |
| train/             |          |
|    actor_loss      | 2.54     |
|    critic_loss     | 0.0752   |
|    ent_coef        | 0.00199  |
|    ent_coef_loss   | 0.0324   |
|    learning_rate   | 0.00073  |
|    n_updates       | 29952    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 460      |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 53       |
|    time_elapsed    | 743      |
|    total_timesteps | 40057    |
| train/             |          |
|    actor_loss      | 2.56     |
|    critic_loss     | 0.077    |
|    ent_coef        | 0.002    |
|    ent_coef_loss   | 0.831    |
|    learning_rate   | 0.00073  |
|    n_updates       | 30016    |
|    std             | 0.042    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 52       |
|    time_elapsed    | 858      |
|    total_timesteps | 44923    |
| train/             |          |
|    actor_loss      | 2.34     |
|    critic_loss     | 0.107    |
|    ent_coef        | 0.00183  |
|    ent_coef_loss   | -0.248   |
|    learning_rate   | 0.00073  |
|    n_updates       | 34880    |
|    std             | 0.0418   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 52       |
|    time_elapsed    | 866      |
|    total_timesteps | 45244    |
| train/             |          |
|    actor_loss      | 2.47     |
|    critic_loss     | 0.146    |
|    ent_coef        | 0.00182  |
|    ent_coef_loss   | 0.506    |
|    learning_rate   | 0.00073  |
|    n_updates       | 35200    |
|    std             | 0.0418   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 460      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 52       |
|    time_elapsed    | 874      |
|    total_timesteps | 45533    |
| train/             |          |
|    actor_loss      | 2.62     |
|    critic_loss     | 0.203    |
|    ent_coef        | 0.00181  |
|    ent_coef_loss   | 0.517    |
|    learning_rate   | 0.00073  |
|    n_updates       | 35520    |
|    std             | 0.0418   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 442      |
|    ep_rew_mean     | -103     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 51       |
|    time_elapsed    | 880      |
|    total_timesteps | 45786    |
| train/             |          |
|    actor_loss      | 2.79     |
|    critic_loss     | 0.26     |
|    ent_coef        | 0.00177  |
|    ent_coef_loss   | -1.19    |
|    learning_rate   | 0.00073  |
|    n_updates       | 35776    |
|    std             | 0.0417   |
---------------------------------
Eval num_timesteps=50000, episode_reward=-33.32 +/- 0.71
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -33.3    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | 3.08     |
|    critic_loss     | 0.11     |
|    ent_coef        | 0.00174  |
|    ent_coef_loss   | -0.625   |
|    learning_rate   | 0.00073  |
|    n_updates       | 40000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | -101     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 50       |
|    time_elapsed    | 1030     |
|    total_timesteps | 51600    |
| train/             |          |
|    actor_loss      | 2.95     |
|    critic_loss     | 0.0938   |
|    ent_coef        | 0.00176  |
|    ent_coef_loss   | -0.393   |
|    learning_rate   | 0.00073  |
|    n_updates       | 41600    |
|    std             | 0.0408   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 505      |
|    ep_rew_mean     | -99.1    |
| time/              |          |
|    episodes        | 100      |
|    fps             | 49       |
|    time_elapsed    | 1151     |
|    total_timesteps | 56519    |
| train/             |          |
|    actor_loss      | 2.84     |
|    critic_loss     | 0.0746   |
|    ent_coef        | 0.00186  |
|    ent_coef_loss   | 1.69     |
|    learning_rate   | 0.00073  |
|    n_updates       | 46528    |
|    std             | 0.0406   |
---------------------------------
Eval num_timesteps=60000, episode_reward=-35.83 +/- 2.48
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -35.8    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | 2.55     |
|    critic_loss     | 0.0904   |
|    ent_coef        | 0.00163  |
|    ent_coef_loss   | -0.271   |
|    learning_rate   | 0.00073  |
|    n_updates       | 49984    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 520      |
|    ep_rew_mean     | -97.7    |
| time/              |          |
|    episodes        | 104      |
|    fps             | 47       |
|    time_elapsed    | 1288     |
|    total_timesteps | 61708    |
| train/             |          |
|    actor_loss      | 2.57     |
|    critic_loss     | 0.0629   |
|    ent_coef        | 0.00161  |
|    ent_coef_loss   | -0.114   |
|    learning_rate   | 0.00073  |
|    n_updates       | 51712    |
|    std             | 0.0403   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 535      |
|    ep_rew_mean     | -95.3    |
| time/              |          |
|    episodes        | 108      |
|    fps             | 47       |
|    time_elapsed    | 1430     |
|    total_timesteps | 68108    |
| train/             |          |
|    actor_loss      | 2.37     |
|    critic_loss     | 0.0555   |
|    ent_coef        | 0.00161  |
|    ent_coef_loss   | -0.578   |
|    learning_rate   | 0.00073  |
|    n_updates       | 58112    |
|    std             | 0.0401   |
---------------------------------
Eval num_timesteps=70000, episode_reward=-84.97 +/- 41.82
Episode length: 462.40 +/- 571.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 462      |
|    mean_reward     | -85      |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | 2.47     |
|    critic_loss     | 0.0537   |
|    ent_coef        | 0.00161  |
|    ent_coef_loss   | 0.743    |
|    learning_rate   | 0.00073  |
|    n_updates       | 59968    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 575      |
|    ep_rew_mean     | -94      |
| time/              |          |
|    episodes        | 112      |
|    fps             | 47       |
|    time_elapsed    | 1536     |
|    total_timesteps | 72674    |
| train/             |          |
|    actor_loss      | 2.54     |
|    critic_loss     | 0.0692   |
|    ent_coef        | 0.00158  |
|    ent_coef_loss   | -0.962   |
|    learning_rate   | 0.00073  |
|    n_updates       | 62656    |
|    std             | 0.0398   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 636      |
|    ep_rew_mean     | -90.4    |
| time/              |          |
|    episodes        | 116      |
|    fps             | 46       |
|    time_elapsed    | 1686     |
|    total_timesteps | 79074    |
| train/             |          |
|    actor_loss      | 2.28     |
|    critic_loss     | 0.0528   |
|    ent_coef        | 0.00156  |
|    ent_coef_loss   | 0.115    |
|    learning_rate   | 0.00073  |
|    n_updates       | 69056    |
|    std             | 0.0389   |
---------------------------------
Eval num_timesteps=80000, episode_reward=-86.65 +/- 44.52
Episode length: 477.60 +/- 564.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 478      |
|    mean_reward     | -86.7    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | 2.12     |
|    critic_loss     | 0.0479   |
|    ent_coef        | 0.00155  |
|    ent_coef_loss   | 0.0168   |
|    learning_rate   | 0.00073  |
|    n_updates       | 69952    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 682      |
|    ep_rew_mean     | -87.4    |
| time/              |          |
|    episodes        | 120      |
|    fps             | 46       |
|    time_elapsed    | 1843     |
|    total_timesteps | 85081    |
| train/             |          |
|    actor_loss      | 2.17     |
|    critic_loss     | 0.0597   |
|    ent_coef        | 0.00153  |
|    ent_coef_loss   | 0.00494  |
|    learning_rate   | 0.00073  |
|    n_updates       | 75072    |
|    std             | 0.0384   |
---------------------------------
Eval num_timesteps=90000, episode_reward=-6.62 +/- 7.19
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -6.62    |
| time/              |          |
|    total_timesteps | 90000    |
| train/             |          |
|    actor_loss      | 2.12     |
|    critic_loss     | 0.0573   |
|    ent_coef        | 0.00145  |
|    ent_coef_loss   | -0.877   |
|    learning_rate   | 0.00073  |
|    n_updates       | 80000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 741      |
|    ep_rew_mean     | -84.3    |
| time/              |          |
|    episodes        | 124      |
|    fps             | 45       |
|    time_elapsed    | 2016     |
|    total_timesteps | 91600    |
| train/             |          |
|    actor_loss      | 2.08     |
|    critic_loss     | 0.0465   |
|    ent_coef        | 0.00137  |
|    ent_coef_loss   | 0.0739   |
|    learning_rate   | 0.00073  |
|    n_updates       | 81600    |
|    std             | 0.0379   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 786      |
|    ep_rew_mean     | -81      |
| time/              |          |
|    episodes        | 128      |
|    fps             | 45       |
|    time_elapsed    | 2132     |
|    total_timesteps | 96484    |
| train/             |          |
|    actor_loss      | 2.01     |
|    critic_loss     | 0.0829   |
|    ent_coef        | 0.0014   |
|    ent_coef_loss   | 0.981    |
|    learning_rate   | 0.00073  |
|    n_updates       | 86464    |
|    std             | 0.0375   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 786      |
|    ep_rew_mean     | -80.8    |
| time/              |          |
|    episodes        | 132      |
|    fps             | 45       |
|    time_elapsed    | 2141     |
|    total_timesteps | 96821    |
| train/             |          |
|    actor_loss      | 1.95     |
|    critic_loss     | 0.122    |
|    ent_coef        | 0.00141  |
|    ent_coef_loss   | -0.0157  |
|    learning_rate   | 0.00073  |
|    n_updates       | 86784    |
|    std             | 0.0375   |
---------------------------------
Eval num_timesteps=100000, episode_reward=-82.56 +/- 0.56
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -82.6    |
| time/              |          |
|    total_timesteps | 100000   |
| train/             |          |
|    actor_loss      | 2.11     |
|    critic_loss     | 0.071    |
|    ent_coef        | 0.00143  |
|    ent_coef_loss   | -0.0611  |
|    learning_rate   | 0.00073  |
|    n_updates       | 89984    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 828      |
|    ep_rew_mean     | -77.4    |
| time/              |          |
|    episodes        | 136      |
|    fps             | 44       |
|    time_elapsed    | 2313     |
|    total_timesteps | 103200   |
| train/             |          |
|    actor_loss      | 2.09     |
|    critic_loss     | 0.073    |
|    ent_coef        | 0.00142  |
|    ent_coef_loss   | 0.252    |
|    learning_rate   | 0.00073  |
|    n_updates       | 93184    |
|    std             | 0.0372   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 889      |
|    ep_rew_mean     | -73.8    |
| time/              |          |
|    episodes        | 140      |
|    fps             | 44       |
|    time_elapsed    | 2476     |
|    total_timesteps | 109600   |
| train/             |          |
|    actor_loss      | 1.98     |
|    critic_loss     | 0.0692   |
|    ent_coef        | 0.0015   |
|    ent_coef_loss   | 0.592    |
|    learning_rate   | 0.00073  |
|    n_updates       | 99584    |
|    std             | 0.0369   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-10.39 +/- 1.99
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -10.4    |
| time/              |          |
|    total_timesteps | 110000   |
| train/             |          |
|    actor_loss      | 1.94     |
|    critic_loss     | 0.0794   |
|    ent_coef        | 0.00149  |
|    ent_coef_loss   | -0.743   |
|    learning_rate   | 0.00073  |
|    n_updates       | 99968    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 936      |
|    ep_rew_mean     | -70.7    |
| time/              |          |
|    episodes        | 144      |
|    fps             | 43       |
|    time_elapsed    | 2638     |
|    total_timesteps | 115220   |
| train/             |          |
|    actor_loss      | 1.91     |
|    critic_loss     | 0.0608   |
|    ent_coef        | 0.00138  |
|    ent_coef_loss   | -1.08    |
|    learning_rate   | 0.00073  |
|    n_updates       | 105216   |
|    std             | 0.0364   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-8.65 +/- 1.93
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -8.65    |
| time/              |          |
|    total_timesteps | 120000   |
| train/             |          |
|    actor_loss      | 1.87     |
|    critic_loss     | 0.0442   |
|    ent_coef        | 0.00142  |
|    ent_coef_loss   | 0.384    |
|    learning_rate   | 0.00073  |
|    n_updates       | 109952   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 989      |
|    ep_rew_mean     | -66.1    |
| time/              |          |
|    episodes        | 148      |
|    fps             | 43       |
|    time_elapsed    | 2852     |
|    total_timesteps | 123200   |
| train/             |          |
|    actor_loss      | 1.71     |
|    critic_loss     | 0.116    |
|    ent_coef        | 0.00138  |
|    ent_coef_loss   | -1.48    |
|    learning_rate   | 0.00073  |
|    n_updates       | 113152   |
|    std             | 0.036    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -63.1    |
| time/              |          |
|    episodes        | 152      |
|    fps             | 42       |
|    time_elapsed    | 3020     |
|    total_timesteps | 129600   |
| train/             |          |
|    actor_loss      | 1.69     |
|    critic_loss     | 0.113    |
|    ent_coef        | 0.00156  |
|    ent_coef_loss   | 0.276    |
|    learning_rate   | 0.00073  |
|    n_updates       | 119552   |
|    std             | 0.0357   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-8.21 +/- 2.41
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -8.21    |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | 1.75     |
|    critic_loss     | 0.0842   |
|    ent_coef        | 0.00152  |
|    ent_coef_loss   | 0.31     |
|    learning_rate   | 0.00073  |
|    n_updates       | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | -60.9    |
| time/              |          |
|    episodes        | 156      |
|    fps             | 42       |
|    time_elapsed    | 3171     |
|    total_timesteps | 135289   |
| train/             |          |
|    actor_loss      | 1.75     |
|    critic_loss     | 0.0539   |
|    ent_coef        | 0.00146  |
|    ent_coef_loss   | 0.69     |
|    learning_rate   | 0.00073  |
|    n_updates       | 125248   |
|    std             | 0.0357   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-4.16 +/- 1.14
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -4.16    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | 1.71     |
|    critic_loss     | 0.0453   |
|    ent_coef        | 0.00143  |
|    ent_coef_loss   | -0.673   |
|    learning_rate   | 0.00073  |
|    n_updates       | 129984   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.08e+03 |
|    ep_rew_mean     | -57      |
| time/              |          |
|    episodes        | 160      |
|    fps             | 42       |
|    time_elapsed    | 3361     |
|    total_timesteps | 143200   |
| train/             |          |
|    actor_loss      | 1.7      |
|    critic_loss     | 0.0447   |
|    ent_coef        | 0.00163  |
|    ent_coef_loss   | -1.7     |
|    learning_rate   | 0.00073  |
|    n_updates       | 133184   |
|    std             | 0.0356   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.15e+03 |
|    ep_rew_mean     | -53.4    |
| time/              |          |
|    episodes        | 164      |
|    fps             | 42       |
|    time_elapsed    | 3531     |
|    total_timesteps | 149600   |
| train/             |          |
|    actor_loss      | 1.55     |
|    critic_loss     | 0.0478   |
|    ent_coef        | 0.00156  |
|    ent_coef_loss   | -0.151   |
|    learning_rate   | 0.00073  |
|    n_updates       | 139584   |
|    std             | 0.0358   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-6.81 +/- 1.95
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -6.81    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | 1.54     |
|    critic_loss     | 0.0392   |
|    ent_coef        | 0.00159  |
|    ent_coef_loss   | 0.674    |
|    learning_rate   | 0.00073  |
|    n_updates       | 139968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.16e+03 |
|    ep_rew_mean     | -50.1    |
| time/              |          |
|    episodes        | 168      |
|    fps             | 42       |
|    time_elapsed    | 3698     |
|    total_timesteps | 156400   |
| train/             |          |
|    actor_loss      | 1.4      |
|    critic_loss     | 0.0433   |
|    ent_coef        | 0.00152  |
|    ent_coef_loss   | -1.23    |
|    learning_rate   | 0.00073  |
|    n_updates       | 146368   |
|    std             | 0.0359   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-6.69 +/- 0.39
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -6.69    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | 1.29     |
|    critic_loss     | 0.0393   |
|    ent_coef        | 0.00149  |
|    ent_coef_loss   | -0.769   |
|    learning_rate   | 0.00073  |
|    n_updates       | 149952   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.18e+03 |
|    ep_rew_mean     | -48.2    |
| time/              |          |
|    episodes        | 172      |
|    fps             | 42       |
|    time_elapsed    | 3829     |
|    total_timesteps | 161600   |
| train/             |          |
|    actor_loss      | 1.47     |
|    critic_loss     | 0.0405   |
|    ent_coef        | 0.00162  |
|    ent_coef_loss   | 0.895    |
|    learning_rate   | 0.00073  |
|    n_updates       | 151552   |
|    std             | 0.036    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.19e+03 |
|    ep_rew_mean     | -46.5    |
| time/              |          |
|    episodes        | 176      |
|    fps             | 42       |
|    time_elapsed    | 3987     |
|    total_timesteps | 168000   |
| train/             |          |
|    actor_loss      | 1.37     |
|    critic_loss     | 0.0398   |
|    ent_coef        | 0.00148  |
|    ent_coef_loss   | -1.64    |
|    learning_rate   | 0.00073  |
|    n_updates       | 157952   |
|    std             | 0.036    |
---------------------------------
Eval num_timesteps=170000, episode_reward=-11.88 +/- 16.83
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -11.9    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | 1.31     |
|    critic_loss     | 0.036    |
|    ent_coef        | 0.00138  |
|    ent_coef_loss   | -0.771   |
|    learning_rate   | 0.00073  |
|    n_updates       | 160000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.21e+03 |
|    ep_rew_mean     | -44.3    |
| time/              |          |
|    episodes        | 180      |
|    fps             | 41       |
|    time_elapsed    | 4173     |
|    total_timesteps | 174800   |
| train/             |          |
|    actor_loss      | 1.22     |
|    critic_loss     | 0.0482   |
|    ent_coef        | 0.00144  |
|    ent_coef_loss   | 2.56     |
|    learning_rate   | 0.00073  |
|    n_updates       | 164800   |
|    std             | 0.036    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.25e+03 |
|    ep_rew_mean     | -41.5    |
| time/              |          |
|    episodes        | 184      |
|    fps             | 41       |
|    time_elapsed    | 4301     |
|    total_timesteps | 179738   |
| train/             |          |
|    actor_loss      | 1.33     |
|    critic_loss     | 0.0447   |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | 0.513    |
|    learning_rate   | 0.00073  |
|    n_updates       | 169728   |
|    std             | 0.0361   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-1.51 +/- 1.41
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -1.51    |
| time/              |          |
|    total_timesteps | 180000   |
| train/             |          |
|    actor_loss      | 1.29     |
|    critic_loss     | 0.102    |
|    ent_coef        | 0.00163  |
|    ent_coef_loss   | 1.22     |
|    learning_rate   | 0.00073  |
|    n_updates       | 169984   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.32e+03 |
|    ep_rew_mean     | -37.9    |
| time/              |          |
|    episodes        | 188      |
|    fps             | 41       |
|    time_elapsed    | 4478     |
|    total_timesteps | 186400   |
| train/             |          |
|    actor_loss      | 1.18     |
|    critic_loss     | 0.0816   |
|    ent_coef        | 0.00162  |
|    ent_coef_loss   | -1.05    |
|    learning_rate   | 0.00073  |
|    n_updates       | 176384   |
|    std             | 0.0362   |
---------------------------------
Eval num_timesteps=190000, episode_reward=1.62 +/- 1.06
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | 1.62     |
| time/              |          |
|    total_timesteps | 190000   |
| train/             |          |
|    actor_loss      | 1.14     |
|    critic_loss     | 0.0592   |
|    ent_coef        | 0.00168  |
|    ent_coef_loss   | 1.27     |
|    learning_rate   | 0.00073  |
|    n_updates       | 179968   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.37e+03 |
|    ep_rew_mean     | -35.4    |
| time/              |          |
|    episodes        | 192      |
|    fps             | 41       |
|    time_elapsed    | 4646     |
|    total_timesteps | 192374   |
| train/             |          |
|    actor_loss      | 1.15     |
|    critic_loss     | 0.0598   |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | 0.094    |
|    learning_rate   | 0.00073  |
|    n_updates       | 182336   |
|    std             | 0.0365   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.38e+03 |
|    ep_rew_mean     | -33.6    |
| time/              |          |
|    episodes        | 196      |
|    fps             | 40       |
|    time_elapsed    | 4858     |
|    total_timesteps | 198774   |
| train/             |          |
|    actor_loss      | 1.24     |
|    critic_loss     | 0.0422   |
|    ent_coef        | 0.0015   |
|    ent_coef_loss   | 0.627    |
|    learning_rate   | 0.00073  |
|    n_updates       | 188736   |
|    std             | 0.0364   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-61.07 +/- 55.72
Episode length: 754.60 +/- 694.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 755      |
|    mean_reward     | -61.1    |
| time/              |          |
|    total_timesteps | 200000   |
| train/             |          |
|    actor_loss      | 1.17     |
|    critic_loss     | 0.0462   |
|    ent_coef        | 0.00137  |
|    ent_coef_loss   | -0.513   |
|    learning_rate   | 0.00073  |
|    n_updates       | 189952   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.39e+03 |
|    ep_rew_mean     | -32.1    |
| time/              |          |
|    episodes        | 200      |
|    fps             | 40       |
|    time_elapsed    | 5028     |
|    total_timesteps | 204800   |
| train/             |          |
|    actor_loss      | 1.18     |
|    critic_loss     | 0.0713   |
|    ent_coef        | 0.00149  |
|    ent_coef_loss   | 0.981    |
|    learning_rate   | 0.00073  |
|    n_updates       | 194752   |
|    std             | 0.036    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.37e+03 |
|    ep_rew_mean     | -32.3    |
| time/              |          |
|    episodes        | 204      |
|    fps             | 40       |
|    time_elapsed    | 5116     |
|    total_timesteps | 208140   |
| train/             |          |
|    actor_loss      | 1.21     |
|    critic_loss     | 0.0844   |
|    ent_coef        | 0.00145  |
|    ent_coef_loss   | 1.44     |
|    learning_rate   | 0.00073  |
|    n_updates       | 198144   |
|    std             | 0.0359   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-74.72 +/- 41.37
Episode length: 467.40 +/- 567.25
---------------------------------
| eval/              |          |
|    mean_ep_length  | 467      |
|    mean_reward     | -74.7    |
| time/              |          |
|    total_timesteps | 210000   |
| train/             |          |
|    actor_loss      | 1.23     |
|    critic_loss     | 0.0781   |
|    ent_coef        | 0.0014   |
|    ent_coef_loss   | 2.33     |
|    learning_rate   | 0.00073  |
|    n_updates       | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.35e+03 |
|    ep_rew_mean     | -33.5    |
| time/              |          |
|    episodes        | 208      |
|    fps             | 40       |
|    time_elapsed    | 5229     |
|    total_timesteps | 212472   |
| train/             |          |
|    actor_loss      | 1.34     |
|    critic_loss     | 0.0955   |
|    ent_coef        | 0.0012   |
|    ent_coef_loss   | -1.47    |
|    learning_rate   | 0.00073  |
|    n_updates       | 202432   |
|    std             | 0.0357   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.33e+03 |
|    ep_rew_mean     | -33.2    |
| time/              |          |
|    episodes        | 212      |
|    fps             | 40       |
|    time_elapsed    | 5290     |
|    total_timesteps | 215001   |
| train/             |          |
|    actor_loss      | 1.47     |
|    critic_loss     | 0.0637   |
|    ent_coef        | 0.00131  |
|    ent_coef_loss   | -0.101   |
|    learning_rate   | 0.00073  |
|    n_updates       | 204992   |
|    std             | 0.0354   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-1.53 +/- 2.33
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -1.53    |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | 1.53     |
|    critic_loss     | 0.0829   |
|    ent_coef        | 0.00142  |
|    ent_coef_loss   | 0.562    |
|    learning_rate   | 0.00073  |
|    n_updates       | 209984   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.33e+03 |
|    ep_rew_mean     | -33      |
| time/              |          |
|    episodes        | 216      |
|    fps             | 40       |
|    time_elapsed    | 5460     |
|    total_timesteps | 221600   |
| train/             |          |
|    actor_loss      | 1.38     |
|    critic_loss     | 0.0585   |
|    ent_coef        | 0.00143  |
|    ent_coef_loss   | 0.631    |
|    learning_rate   | 0.00073  |
|    n_updates       | 211584   |
|    std             | 0.035    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.35e+03 |
|    ep_rew_mean     | -31.9    |
| time/              |          |
|    episodes        | 220      |
|    fps             | 40       |
|    time_elapsed    | 5630     |
|    total_timesteps | 228000   |
| train/             |          |
|    actor_loss      | 1.42     |
|    critic_loss     | 0.0648   |
|    ent_coef        | 0.00147  |
|    ent_coef_loss   | -2.55    |
|    learning_rate   | 0.00073  |
|    n_updates       | 217984   |
|    std             | 0.0346   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-3.21 +/- 0.46
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -3.21    |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | 1.27     |
|    critic_loss     | 0.0472   |
|    ent_coef        | 0.00149  |
|    ent_coef_loss   | -0.389   |
|    learning_rate   | 0.00073  |
|    n_updates       | 219968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.35e+03 |
|    ep_rew_mean     | -30.8    |
| time/              |          |
|    episodes        | 224      |
|    fps             | 30       |
|    time_elapsed    | 7796     |
|    total_timesteps | 234800   |
| train/             |          |
|    actor_loss      | 1.18     |
|    critic_loss     | 0.0355   |
|    ent_coef        | 0.0015   |
|    ent_coef_loss   | 0.437    |
|    learning_rate   | 0.00073  |
|    n_updates       | 224768   |
|    std             | 0.0344   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.33e+03 |
|    ep_rew_mean     | -31.6    |
| time/              |          |
|    episodes        | 228      |
|    fps             | 30       |
|    time_elapsed    | 7883     |
|    total_timesteps | 238438   |
| train/             |          |
|    actor_loss      | 1.27     |
|    critic_loss     | 0.0427   |
|    ent_coef        | 0.00135  |
|    ent_coef_loss   | -1.84    |
|    learning_rate   | 0.00073  |
|    n_updates       | 228416   |
|    std             | 0.0344   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-78.89 +/- 30.46
Episode length: 376.40 +/- 302.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 376      |
|    mean_reward     | -78.9    |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | 1.33     |
|    critic_loss     | 0.0683   |
|    ent_coef        | 0.00152  |
|    ent_coef_loss   | -1.59    |
|    learning_rate   | 0.00073  |
|    n_updates       | 229952   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.35e+03 |
|    ep_rew_mean     | -30.4    |
| time/              |          |
|    episodes        | 232      |
|    fps             | 30       |
|    time_elapsed    | 7939     |
|    total_timesteps | 240793   |
| train/             |          |
|    actor_loss      | 1.32     |
|    critic_loss     | 0.0631   |
|    ent_coef        | 0.00145  |
|    ent_coef_loss   | 0.644    |
|    learning_rate   | 0.00073  |
|    n_updates       | 230784   |
|    std             | 0.0344   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.35e+03 |
|    ep_rew_mean     | -31.7    |
| time/              |          |
|    episodes        | 236      |
|    fps             | 30       |
|    time_elapsed    | 8062     |
|    total_timesteps | 246093   |
| train/             |          |
|    actor_loss      | 1.39     |
|    critic_loss     | 0.0537   |
|    ent_coef        | 0.00147  |
|    ent_coef_loss   | -0.948   |
|    learning_rate   | 0.00073  |
|    n_updates       | 236096   |
|    std             | 0.0342   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-27.94 +/- 35.30
Episode length: 1338.20 +/- 523.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.34e+03 |
|    mean_reward     | -27.9    |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | 1.27     |
|    critic_loss     | 0.0527   |
|    ent_coef        | 0.00136  |
|    ent_coef_loss   | -1.01    |
|    learning_rate   | 0.00073  |
|    n_updates       | 240000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.35e+03 |
|    ep_rew_mean     | -30.3    |
| time/              |          |
|    episodes        | 240      |
|    fps             | 30       |
|    time_elapsed    | 8237     |
|    total_timesteps | 253200   |
| train/             |          |
|    actor_loss      | 1.2      |
|    critic_loss     | 0.0533   |
|    ent_coef        | 0.00142  |
|    ent_coef_loss   | 0.0574   |
|    learning_rate   | 0.00073  |
|    n_updates       | 243200   |
|    std             | 0.0338   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.34e+03 |
|    ep_rew_mean     | -29.6    |
| time/              |          |
|    episodes        | 244      |
|    fps             | 30       |
|    time_elapsed    | 8340     |
|    total_timesteps | 257577   |
| train/             |          |
|    actor_loss      | 1.17     |
|    critic_loss     | 0.0673   |
|    ent_coef        | 0.00136  |
|    ent_coef_loss   | -0.746   |
|    learning_rate   | 0.00073  |
|    n_updates       | 247552   |
|    std             | 0.0337   |
---------------------------------
Eval num_timesteps=260000, episode_reward=-33.05 +/- 30.95
Episode length: 1322.80 +/- 554.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.32e+03 |
|    mean_reward     | -33.1    |
| time/              |          |
|    total_timesteps | 260000   |
| train/             |          |
|    actor_loss      | 1.15     |
|    critic_loss     | 0.0493   |
|    ent_coef        | 0.00142  |
|    ent_coef_loss   | 0.185    |
|    learning_rate   | 0.00073  |
|    n_updates       | 249984   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.33e+03 |
|    ep_rew_mean     | -30.3    |
| time/              |          |
|    episodes        | 248      |
|    fps             | 31       |
|    time_elapsed    | 8488     |
|    total_timesteps | 263613   |
| train/             |          |
|    actor_loss      | 1.32     |
|    critic_loss     | 0.0628   |
|    ent_coef        | 0.00155  |
|    ent_coef_loss   | -0.282   |
|    learning_rate   | 0.00073  |
|    n_updates       | 253568   |
|    std             | 0.0338   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.3e+03  |
|    ep_rew_mean     | -29.8    |
| time/              |          |
|    episodes        | 252      |
|    fps             | 31       |
|    time_elapsed    | 8576     |
|    total_timesteps | 267218   |
| train/             |          |
|    actor_loss      | 1.21     |
|    critic_loss     | 0.0549   |
|    ent_coef        | 0.00155  |
|    ent_coef_loss   | -0.0547  |
|    learning_rate   | 0.00073  |
|    n_updates       | 257216   |
|    std             | 0.034    |
---------------------------------
Eval num_timesteps=270000, episode_reward=-12.60 +/- 37.78
Episode length: 1600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.6e+03  |
|    mean_reward     | -12.6    |
| time/              |          |
|    total_timesteps | 270000   |
| train/             |          |
|    actor_loss      | 1.23     |
|    critic_loss     | 0.0634   |
|    ent_coef        | 0.00158  |
|    ent_coef_loss   | 0.546    |
|    learning_rate   | 0.00073  |
|    n_updates       | 259968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.27e+03 |
|    ep_rew_mean     | -31.3    |
| time/              |          |
|    episodes        | 256      |
|    fps             | 31       |
|    time_elapsed    | 8650     |
|    total_timesteps | 270096   |
| train/             |          |
|    actor_loss      | 1.37     |
|    critic_loss     | 0.0715   |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | -0.764   |
|    learning_rate   | 0.00073  |
|    n_updates       | 260096   |
|    std             | 0.0341   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.27e+03 |
|    ep_rew_mean     | -29.3    |
| time/              |          |
|    episodes        | 260      |
|    fps             | 31       |
|    time_elapsed    | 8800     |
|    total_timesteps | 276496   |
| train/             |          |
|    actor_loss      | 1.12     |
|    critic_loss     | 0.0547   |
|    ent_coef        | 0.00175  |
|    ent_coef_loss   | 0.219    |
|    learning_rate   | 0.00073  |
|    n_updates       | 266496   |
|    std             | 0.0345   |
---------------------------------
Eval num_timesteps=280000, episode_reward=-90.41 +/- 3.50
Episode length: 120.40 +/- 18.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 120      |
|    mean_reward     | -90.4    |
| time/              |          |
|    total_timesteps | 280000   |
| train/             |          |
|    actor_loss      | 1.2      |
|    critic_loss     | 0.0624   |
|    ent_coef        | 0.00185  |
|    ent_coef_loss   | 0.0675   |
|    learning_rate   | 0.00073  |
|    n_updates       | 269952   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.24e+03 |
|    ep_rew_mean     | -29.1    |
| time/              |          |
|    episodes        | 264      |
|    fps             | 31       |
|    time_elapsed    | 8907     |
|    total_timesteps | 280857   |
| train/             |          |
|    actor_loss      | 1.07     |
|    critic_loss     | 0.0641   |
|    ent_coef        | 0.00168  |
|    ent_coef_loss   | -0.39    |
|    learning_rate   | 0.00073  |
|    n_updates       | 270848   |
|    std             | 0.0345   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.22e+03 |
|    ep_rew_mean     | -27.9    |
| time/              |          |
|    episodes        | 268      |
|    fps             | 31       |
|    time_elapsed    | 9002     |
|    total_timesteps | 284856   |
| train/             |          |
|    actor_loss      | 0.911    |
|    critic_loss     | 0.0884   |
|    ent_coef        | 0.00198  |
|    ent_coef_loss   | -1.32    |
|    learning_rate   | 0.00073  |
|    n_updates       | 274816   |
|    std             | 0.0343   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.17e+03 |
|    ep_rew_mean     | -29.7    |
| time/              |          |
|    episodes        | 272      |
|    fps             | 31       |
|    time_elapsed    | 9015     |
|    total_timesteps | 285437   |
| train/             |          |
|    actor_loss      | 1.06     |
|    critic_loss     | 0.0985   |
|    ent_coef        | 0.00182  |
|    ent_coef_loss   | -1.4     |
|    learning_rate   | 0.00073  |
|    n_updates       | 275392   |
|    std             | 0.0343   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.13e+03 |
|    ep_rew_mean     | -31.5    |
| time/              |          |
|    episodes        | 276      |
|    fps             | 31       |
|    time_elapsed    | 9070     |
|    total_timesteps | 287702   |
| train/             |          |
|    actor_loss      | 1        |
|    critic_loss     | 0.101    |
|    ent_coef        | 0.00164  |
|    ent_coef_loss   | -2.18    |
|    learning_rate   | 0.00073  |
|    n_updates       | 277696   |
|    std             | 0.0341   |
---------------------------------
Eval num_timesteps=290000, episode_reward=46.05 +/- 81.52
Episode length: 1356.60 +/- 486.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.36e+03 |
|    mean_reward     | 46       |
| time/              |          |
|    total_timesteps | 290000   |
| train/             |          |
|    actor_loss      | 1.2      |
|    critic_loss     | 0.125    |
|    ent_coef        | 0.00149  |
|    ent_coef_loss   | 1.72     |
|    learning_rate   | 0.00073  |
|    n_updates       | 280000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.11e+03 |
|    ep_rew_mean     | -29.7    |
| time/              |          |
|    episodes        | 280      |
|    fps             | 31       |
|    time_elapsed    | 9170     |
|    total_timesteps | 291600   |
| train/             |          |
|    actor_loss      | 1.1      |
|    critic_loss     | 0.0958   |
|    ent_coef        | 0.0017   |
|    ent_coef_loss   | -0.317   |
|    learning_rate   | 0.00073  |
|    n_updates       | 281600   |
|    std             | 0.034    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.12e+03 |
|    ep_rew_mean     | -23.4    |
| time/              |          |
|    episodes        | 284      |
|    fps             | 31       |
|    time_elapsed    | 9324     |
|    total_timesteps | 298000   |
| train/             |          |
|    actor_loss      | 0.997    |
|    critic_loss     | 0.078    |
|    ent_coef        | 0.00183  |
|    ent_coef_loss   | 0.519    |
|    learning_rate   | 0.00073  |
|    n_updates       | 288000   |
|    std             | 0.0338   |
---------------------------------
Eval num_timesteps=300000, episode_reward=190.64 +/- 84.35
Episode length: 1363.40 +/- 231.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.36e+03 |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | 0.874    |
|    critic_loss     | 0.0933   |
|    ent_coef        | 0.00185  |
|    ent_coef_loss   | 0.726    |
|    learning_rate   | 0.00073  |
|    n_updates       | 289984   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.1e+03  |
|    ep_rew_mean     | -19.8    |
| time/              |          |
|    episodes        | 288      |
|    fps             | 32       |
|    time_elapsed    | 9431     |
|    total_timesteps | 302289   |
| train/             |          |
|    actor_loss      | 0.676    |
|    critic_loss     | 0.084    |
|    ent_coef        | 0.00185  |
|    ent_coef_loss   | -0.587   |
|    learning_rate   | 0.00073  |
|    n_updates       | 292288   |
|    std             | 0.0339   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.08e+03 |
|    ep_rew_mean     | -16      |
| time/              |          |
|    episodes        | 292      |
|    fps             | 32       |
|    time_elapsed    | 9540     |
|    total_timesteps | 306714   |
| train/             |          |
|    actor_loss      | 0.912    |
|    critic_loss     | 0.142    |
|    ent_coef        | 0.00185  |
|    ent_coef_loss   | 0.716    |
|    learning_rate   | 0.00073  |
|    n_updates       | 296704   |
|    std             | 0.0338   |
---------------------------------
Eval num_timesteps=310000, episode_reward=178.48 +/- 121.74
Episode length: 1320.20 +/- 559.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.32e+03 |
|    mean_reward     | 178      |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | 0.772    |
|    critic_loss     | 0.102    |
|    ent_coef        | 0.00203  |
|    ent_coef_loss   | -0.0404  |
|    learning_rate   | 0.00073  |
|    n_updates       | 299968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.08e+03 |
|    ep_rew_mean     | -7.86    |
| time/              |          |
|    episodes        | 296      |
|    fps             | 32       |
|    time_elapsed    | 9703     |
|    total_timesteps | 313200   |
| train/             |          |
|    actor_loss      | 0.286    |
|    critic_loss     | 0.105    |
|    ent_coef        | 0.00187  |
|    ent_coef_loss   | 1.69     |
|    learning_rate   | 0.00073  |
|    n_updates       | 303168   |
|    std             | 0.0336   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.09e+03 |
|    ep_rew_mean     | 2.18     |
| time/              |          |
|    episodes        | 300      |
|    fps             | 32       |
|    time_elapsed    | 9843     |
|    total_timesteps | 318945   |
| train/             |          |
|    actor_loss      | 0.0325   |
|    critic_loss     | 0.113    |
|    ent_coef        | 0.00282  |
|    ent_coef_loss   | -0.803   |
|    learning_rate   | 0.00073  |
|    n_updates       | 308928   |
|    std             | 0.0347   |
---------------------------------
Eval num_timesteps=320000, episode_reward=135.57 +/- 84.38
Episode length: 933.80 +/- 201.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 934      |
|    mean_reward     | 136      |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -0.0983  |
|    critic_loss     | 0.0929   |
|    ent_coef        | 0.00197  |
|    ent_coef_loss   | -3.6     |
|    learning_rate   | 0.00073  |
|    n_updates       | 309952   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.09e+03 |
|    ep_rew_mean     | 10.8     |
| time/              |          |
|    episodes        | 304      |
|    fps             | 32       |
|    time_elapsed    | 9970     |
|    total_timesteps | 323931   |
| train/             |          |
|    actor_loss      | -0.419   |
|    critic_loss     | 0.0968   |
|    ent_coef        | 0.00188  |
|    ent_coef_loss   | -0.618   |
|    learning_rate   | 0.00073  |
|    n_updates       | 313920   |
|    std             | 0.0344   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.09e+03 |
|    ep_rew_mean     | 18.9     |
| time/              |          |
|    episodes        | 308      |
|    fps             | 32       |
|    time_elapsed    | 10052    |
|    total_timesteps | 327357   |
| train/             |          |
|    actor_loss      | -1.51    |
|    critic_loss     | 0.0855   |
|    ent_coef        | 0.0056   |
|    ent_coef_loss   | -0.925   |
|    learning_rate   | 0.00073  |
|    n_updates       | 317312   |
|    std             | 0.0355   |
---------------------------------
Eval num_timesteps=330000, episode_reward=283.59 +/- 2.19
Episode length: 1338.00 +/- 31.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.34e+03 |
|    mean_reward     | 284      |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -1.31    |
|    critic_loss     | 0.101    |
|    ent_coef        | 0.0052   |
|    ent_coef_loss   | 1.64     |
|    learning_rate   | 0.00073  |
|    n_updates       | 320000   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.11e+03 |
|    ep_rew_mean     | 30.4     |
| time/              |          |
|    episodes        | 312      |
|    fps             | 32       |
|    time_elapsed    | 10190    |
|    total_timesteps | 332757   |
| train/             |          |
|    actor_loss      | -1.18    |
|    critic_loss     | 0.0726   |
|    ent_coef        | 0.0052   |
|    ent_coef_loss   | 0.568    |
|    learning_rate   | 0.00073  |
|    n_updates       | 322752   |
|    std             | 0.0361   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.09e+03 |
|    ep_rew_mean     | 42.3     |
| time/              |          |
|    episodes        | 316      |
|    fps             | 32       |
|    time_elapsed    | 10308    |
|    total_timesteps | 337620   |
| train/             |          |
|    actor_loss      | -2.02    |
|    critic_loss     | 0.057    |
|    ent_coef        | 0.00533  |
|    ent_coef_loss   | -0.744   |
|    learning_rate   | 0.00073  |
|    n_updates       | 327616   |
|    std             | 0.0363   |
---------------------------------
Eval num_timesteps=340000, episode_reward=288.63 +/- 0.31
Episode length: 1150.20 +/- 29.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1.15e+03 |
|    mean_reward     | 289      |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -1.78    |
|    critic_loss     | 0.0786   |
|    ent_coef        | 0.00392  |
|    ent_coef_loss   | 0.125    |
|    learning_rate   | 0.00073  |
|    n_updates       | 329984   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.06e+03 |
|    ep_rew_mean     | 48.5     |
| time/              |          |
|    episodes        | 320      |
|    fps             | 32       |
|    time_elapsed    | 10401    |
|    total_timesteps | 341210   |
| train/             |          |
|    actor_loss      | -1.93    |
|    critic_loss     | 0.0659   |
|    ent_coef        | 0.00464  |
|    ent_coef_loss   | 0.553    |
|    learning_rate   | 0.00073  |
|    n_updates       | 331200   |
|    std             | 0.0363   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.04e+03 |
|    ep_rew_mean     | 60.5     |
| time/              |          |
|    episodes        | 324      |
|    fps             | 32       |
|    time_elapsed    | 10513    |
|    total_timesteps | 345728   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.0849   |
|    ent_coef        | 0.00515  |
|    ent_coef_loss   | -0.325   |
|    learning_rate   | 0.00073  |
|    n_updates       | 335680   |
|    std             | 0.0364   |
---------------------------------
Eval num_timesteps=350000, episode_reward=150.90 +/- 165.70
Episode length: 824.20 +/- 482.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 824      |
|    mean_reward     | 151      |
| time/              |          |
|    total_timesteps | 350000   |
| train/             |          |
|    actor_loss      | -2.62    |
|    critic_loss     | 0.0728   |
|    ent_coef        | 0.00466  |
|    ent_coef_loss   | -1.23    |
|    learning_rate   | 0.00073  |
|    n_updates       | 339968   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.05e+03 |
|    ep_rew_mean     | 74.4     |
| time/              |          |
|    episodes        | 328      |
|    fps             | 32       |
|    time_elapsed    | 10652    |
|    total_timesteps | 351171   |
| train/             |          |
|    actor_loss      | -2.9     |
|    critic_loss     | 0.0625   |
|    ent_coef        | 0.00436  |
|    ent_coef_loss   | -1.87    |
|    learning_rate   | 0.00073  |
|    n_updates       | 341184   |
|    std             | 0.0365   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.07e+03 |
|    ep_rew_mean     | 87.9     |
| time/              |          |
|    episodes        | 332      |
|    fps             | 33       |
|    time_elapsed    | 10744    |
|    total_timesteps | 354907   |
| train/             |          |
|    actor_loss      | -3.32    |
|    critic_loss     | 0.06     |
|    ent_coef        | 0.004    |
|    ent_coef_loss   | -1.05    |
|    learning_rate   | 0.00073  |
|    n_updates       | 344896   |
|    std             | 0.0364   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.06e+03 |
|    ep_rew_mean     | 103      |
| time/              |          |
|    episodes        | 336      |
|    fps             | 33       |
|    time_elapsed    | 10839    |
|    total_timesteps | 358814   |
| train/             |          |
|    actor_loss      | -3.61    |
|    critic_loss     | 0.0793   |
|    ent_coef        | 0.00546  |
|    ent_coef_loss   | 0.233    |
|    learning_rate   | 0.00073  |
|    n_updates       | 348800   |
|    std             | 0.0364   |
---------------------------------
Eval num_timesteps=360000, episode_reward=294.05 +/- 1.51
Episode length: 958.80 +/- 20.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 959      |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 360000   |
| train/             |          |
|    actor_loss      | -3.76    |
|    critic_loss     | 0.077    |
|    ent_coef        | 0.00494  |
|    ent_coef_loss   | -2.75    |
|    learning_rate   | 0.00073  |
|    n_updates       | 349952   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.03e+03 |
|    ep_rew_mean     | 112      |
| time/              |          |
|    episodes        | 340      |
|    fps             | 33       |
|    time_elapsed    | 10947    |
|    total_timesteps | 362923   |
| train/             |          |
|    actor_loss      | -3.87    |
|    critic_loss     | 0.0906   |
|    ent_coef        | 0.00406  |
|    ent_coef_loss   | -1.36    |
|    learning_rate   | 0.00073  |
|    n_updates       | 352896   |
|    std             | 0.0364   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.02e+03 |
|    ep_rew_mean     | 121      |
| time/              |          |
|    episodes        | 344      |
|    fps             | 33       |
|    time_elapsed    | 11030    |
|    total_timesteps | 366204   |
| train/             |          |
|    actor_loss      | -3.52    |
|    critic_loss     | 0.085    |
|    ent_coef        | 0.0045   |
|    ent_coef_loss   | -0.0331  |
|    learning_rate   | 0.00073  |
|    n_updates       | 356160   |
|    std             | 0.0363   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 128      |
| time/              |          |
|    episodes        | 348      |
|    fps             | 33       |
|    time_elapsed    | 11107    |
|    total_timesteps | 369375   |
| train/             |          |
|    actor_loss      | -3.6     |
|    critic_loss     | 0.0976   |
|    ent_coef        | 0.0052   |
|    ent_coef_loss   | -1.49    |
|    learning_rate   | 0.00073  |
|    n_updates       | 359360   |
|    std             | 0.0364   |
---------------------------------
Eval num_timesteps=370000, episode_reward=301.50 +/- 0.79
Episode length: 968.40 +/- 13.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 968      |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 370000   |
| train/             |          |
|    actor_loss      | -3.72    |
|    critic_loss     | 0.0972   |
|    ent_coef        | 0.00487  |
|    ent_coef_loss   | -0.589   |
|    learning_rate   | 0.00073  |
|    n_updates       | 360000   |
---------------------------------
New best mean reward!
Stopping training because the mean reward 301.50  is above the threshold 300
Training complete. Model saved.
Plotting sample efficiency...
Evaluating model...
Seed 0: mean_reward:297.93 +/- 32.34
